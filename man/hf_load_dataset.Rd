% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/datasets_api.R
\name{hf_load_dataset}
\alias{hf_load_dataset}
\title{Load Dataset via Hugging Face Datasets Server API}
\usage{
hf_load_dataset(
  dataset,
  split = "train",
  config = NULL,
  limit = 1000,
  offset = 0,
  token = NULL
)
}
\arguments{
\item{dataset}{Character string. Dataset name (e.g., "imdb", "squad").}

\item{split}{Character string. Dataset split: "train", "test", "validation", etc.
Default: "train".}

\item{config}{Character string or NULL. Dataset configuration/subset name.
If NULL (default), auto-detected from the dataset's available configs.}

\item{limit}{Integer. Maximum number of rows to fetch. Default: 1000.
Set to Inf to fetch all rows (may be slow for large datasets).}

\item{offset}{Integer. Row offset for pagination. Default: 0.}

\item{token}{Character string or NULL. API token for private datasets.}
}
\value{
A tibble with dataset rows, plus .dataset and .split columns.
}
\description{
Load a dataset from Hugging Face Hub using the Datasets Server API.
This is an API-first approach that doesn't require Python.
For local dataset loading with Python, see the legacy function or advanced vignette.
}
\examples{
\dontrun{
# Load first 1000 rows of IMDB train set
imdb <- hf_load_dataset("imdb", split = "train", limit = 1000)

# Load test set
imdb_test <- hf_load_dataset("imdb", split = "test", limit = 500)
}
}
