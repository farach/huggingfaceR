% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classify-batch.R
\name{hf_classify_chunks}
\alias{hf_classify_chunks}
\title{Chunked Text Classification (Disk Checkpoints)}
\usage{
hf_classify_chunks(
  text,
  output_dir,
  model = "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
  token = NULL,
  chunk_size = 1000L,
  batch_size = 100L,
  max_active = 10L,
  resume = TRUE,
  progress = TRUE
)
}
\arguments{
\item{text}{Character vector of text(s) to classify.}

\item{output_dir}{Character string. Directory to write chunk files.}

\item{model}{Character string. Model ID from Hugging Face Hub.
Default: "distilbert/distilbert-base-uncased-finetuned-sst-2-english".}

\item{token}{Character string or NULL. API token for authentication.}

\item{chunk_size}{Integer. Number of texts per disk chunk. Default: 1000.}

\item{batch_size}{Integer. Number of texts per API request. Default: 100.}

\item{max_active}{Integer. Maximum concurrent requests. Default: 10.}

\item{resume}{Logical. Skip already-completed chunks. Default: TRUE.}

\item{progress}{Logical. Show progress bar. Default: TRUE.}
}
\value{
Invisibly returns the output directory path. Use `hf_read_chunks()`
  to read results.
}
\description{
Classify large datasets with automatic checkpointing to disk.
Supports resuming interrupted processing.
}
\examples{
\dontrun{
# Process large dataset with checkpoints
texts <- rep("sample text", 5000)
hf_classify_chunks(texts, output_dir = "classify_output", chunk_size = 1000)

# Read results
results <- hf_read_chunks("classify_output")
}
}
