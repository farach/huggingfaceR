% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/embeddings-batch.R
\name{hf_embed_chunks}
\alias{hf_embed_chunks}
\title{Chunked Embedding Generation (Disk Checkpoints)}
\usage{
hf_embed_chunks(
  text,
  output_dir,
  model = "BAAI/bge-small-en-v1.5",
  token = NULL,
  chunk_size = 1000L,
  batch_size = 100L,
  max_active = 10L,
  resume = TRUE,
  progress = TRUE
)
}
\arguments{
\item{text}{Character vector of text(s) to embed.}

\item{output_dir}{Character string. Directory to write chunk files.}

\item{model}{Character string. Model ID from Hugging Face Hub.
Default: "BAAI/bge-small-en-v1.5".}

\item{token}{Character string or NULL. API token for authentication.}

\item{chunk_size}{Integer. Number of texts per disk chunk. Default: 1000.}

\item{batch_size}{Integer. Number of texts per API request. Default: 100.}

\item{max_active}{Integer. Maximum concurrent requests. Default: 10.}

\item{resume}{Logical. Skip already-completed chunks. Default: TRUE.}

\item{progress}{Logical. Show progress bar. Default: TRUE.}
}
\value{
Invisibly returns the output directory path. Use `hf_read_chunks()`
  to read results.
}
\description{
Generate embeddings for large datasets with automatic checkpointing to disk.
Supports resuming interrupted processing.
}
\examples{
\dontrun{
# Process large dataset with checkpoints
texts <- rep("sample text", 5000)
hf_embed_chunks(texts, output_dir = "embeddings_output", chunk_size = 1000)

# Read results
results <- hf_read_chunks("embeddings_output")

# Resume interrupted processing
hf_embed_chunks(more_texts, output_dir = "embeddings_output", resume = TRUE)
}
}
