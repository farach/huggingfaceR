% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chat.R
\name{hf_chat}
\alias{hf_chat}
\title{LLM Chat Interface}
\usage{
hf_chat(
  message,
  system = NULL,
  model = "HuggingFaceTB/SmolLM3-3B",
  max_tokens = 500,
  temperature = 0.7,
  token = NULL,
  ...
)
}
\arguments{
\item{message}{Character string. The user message to send to the model.}

\item{system}{Character string or NULL. Optional system prompt to set behavior.}

\item{model}{Character string. Model ID from Hugging Face Hub.
Default: "HuggingFaceTB/SmolLM3-3B". Use `:provider` suffix to select
a specific provider (e.g., "meta-llama/Llama-3-8B-Instruct:together").}

\item{max_tokens}{Integer. Maximum tokens to generate. Default: 500.}

\item{temperature}{Numeric. Sampling temperature (0-2). Default: 0.7.}

\item{token}{Character string or NULL. API token for authentication.}

\item{...}{Additional parameters passed to the model.}
}
\value{
A tibble with columns: role, content, model, tokens_used
}
\description{
Have a conversation with an open-source language model via the Inference Providers API.
}
\examples{
\dontrun{
# Simple question
hf_chat("What is the capital of France?")

# With system prompt
hf_chat(
  "Explain gradient descent",
  system = "You are a statistics professor. Use simple analogies."
)

# Use a specific provider
hf_chat("Hello!", model = "meta-llama/Llama-3-8B-Instruct:together")
}
}
