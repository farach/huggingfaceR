% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ez.R
\name{hf_ez_question_answering_api_inference}
\alias{hf_ez_question_answering_api_inference}
\title{Question Answering API Inference}
\usage{
hf_ez_question_answering_api_inference(
  question,
  context,
  tidy = TRUE,
  use_gpu = FALSE,
  use_cache = FALSE,
  wait_for_model = FALSE,
  use_auth_token = NULL,
  stop_on_error = FALSE,
  ...
)
}
\arguments{
\item{question}{a question to be answered based on the provided context}

\item{context}{the context to consult for answering the question}

\item{tidy}{Whether to tidy the results into a tibble. Default: TRUE (tidy the results)}

\item{use_gpu}{Whether to use GPU for inference.}

\item{use_cache}{Whether to use cached inference results for previously seen inputs.}

\item{wait_for_model}{Whether to wait for the model to be ready instead of receiving a 503 error after a certain amount of time.}

\item{use_auth_token}{The token to use as HTTP bearer authorization for the Inference API. Defaults to HUGGING_FACE_HUB_TOKEN environment variable.}

\item{stop_on_error}{Whether to throw an error if an API error is encountered. Defaults to FALSE (do not throw error).}
}
\value{
The results of the inference
}
\description{
Question Answering API Inference
}
\seealso{
\url{https://huggingface.co/docs/api-inference/index}
}
