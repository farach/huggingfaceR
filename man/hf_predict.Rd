% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/inference.R
\name{hf_predict}
\alias{hf_predict}
\title{Predict with Pipeline}
\usage{
hf_predict(
  pipeline,
  inputs,
  parameters = NULL,
  use_gpu = FALSE,
  use_cache = FALSE,
  wait_for_model = FALSE,
  use_auth_token = NULL,
  ...
)
}
\arguments{
\item{pipeline}{Either a downloaded pipeline from the Hugging Face Hub (using hf_load_model()) or a model_id.}

\item{inputs}{The data to predict on.}

\item{parameters}{Model parameters distinct to the model being used.}

\item{use_gpu}{API Only - Whether to use GPU for inference.}

\item{use_cache}{API Only - Whether to use cached inference results for previously seen inputs.}

\item{wait_for_model}{API Only - Whether to wait for the model to be ready instead of receiving a 503 error after a certain amount of time.}

\item{use_auth_token}{API Only - The token to use as HTTP bearer authorization for the Inference API. Defaults to HUGGING_FACE_HUB_TOKEN environment variable.}

\item{...}{sent to instantiation of model}
}
\value{
A Hugging Face model prediction.
}
\description{
Predict using Hugging Face Pipeline If a model_id is provided, the Inference API will be used to make the prediction.
If you wish to download a pipeline rather than running your predictions through the Inference API, download the model first with the hf_load_pipeline() function.
}
\seealso{
\url{https://huggingface.co/docs/api-inference/index}
}
