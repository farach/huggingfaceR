<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chat, Conversations, and Text Generation • huggingfaceR</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Inter-0.4.10/font.css" rel="stylesheet">
<link href="../deps/JetBrains_Mono-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Chat, Conversations, and Text Generation">
<link rel="icon" type="image/svg+xml" href="favicon/favicon.svg">
<meta property="og:image" content="https://farach.github.io/huggingfaceR/logo.svg">
<meta name="twitter:card" content="summary">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">huggingfaceR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.0.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-book-open"></span> Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><h6 class="dropdown-header" data-toc-skip>Getting Started</h6></li>
    <li><a class="dropdown-item" href="../articles/getting-started.html">Getting Started with huggingfaceR</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Core Capabilities</h6></li>
    <li><a class="dropdown-item" href="../articles/text-classification.html">Text Classification</a></li>
    <li><a class="dropdown-item" href="../articles/embeddings-and-similarity.html">Embeddings and Similarity</a></li>
    <li><a class="dropdown-item" href="../articles/llm-chat-and-generation.html">Chat and Generation</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Advanced Workflows</h6></li>
    <li><a class="dropdown-item" href="../articles/hub-datasets-and-modeling.html">Hub, Datasets, and Modeling</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Case Studies</h6></li>
    <li><a class="dropdown-item" href="../articles/anthropic-economic-index.html">Anthropic Economic Index</a></li>
    <li><a class="dropdown-item" href="../articles/openai-gdpval-benchmark.html">OpenAI GDPval Benchmark</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/farach/huggingfaceR/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Chat, Conversations, and Text Generation</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/farach/huggingfaceR/blob/main/vignettes/llm-chat-and-generation.Rmd" class="external-link"><code>vignettes/llm-chat-and-generation.Rmd</code></a></small>
      <div class="d-none name"><code>llm-chat-and-generation.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">huggingfaceR</span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>huggingfaceR provides access to open-source large language models
(LLMs) through the Hugging Face Inference Providers API. You can ask
questions, hold multi-turn conversations, generate text continuations,
and probe masked language models – all without downloading model weights
or managing GPU resources.</p>
<p>The default model for chat and generation is
<code>HuggingFaceTB/SmolLM3-3B</code>, a compact yet capable open-source
model. You can substitute any chat-compatible model available on the
Hub.</p>
</div>
<div class="section level2">
<h2 id="single-turn-chat-with-hf_chat">Single-Turn Chat with hf_chat()<a class="anchor" aria-label="anchor" href="#single-turn-chat-with-hf_chat"></a>
</h2>
<div class="section level3">
<h3 id="basic-question-answer">Basic Question-Answer<a class="anchor" aria-label="anchor" href="#basic-question-answer"></a>
</h3>
<p><code><a href="../reference/hf_chat.html">hf_chat()</a></code> sends a single message to a language model and
returns the response as a tibble.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span><span class="st">"What are the main differences between R and Python for data analysis?"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 1 x 4</span></span>
<span><span class="co">#&gt;   role      content                                        model         tokens_used</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;     &lt;chr&gt;                                          &lt;chr&gt;               &lt;int&gt;</span></span>
<span><span class="co">#&gt; 1 assistant R and Python are both popular for data anal... HuggingFace...        127</span></span></code></pre></div>
<p>The returned tibble includes the model’s response
(<code>content</code>), the model identifier, and the number of tokens
consumed.</p>
</div>
<div class="section level3">
<h3 id="system-prompts">System Prompts<a class="anchor" aria-label="anchor" href="#system-prompts"></a>
</h3>
<p>System prompts define the model’s behavior, personality, or domain
expertise. They are sent before the user message and persist for the
duration of the request.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Act as a domain expert</span></span>
<span><span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span></span>
<span>  <span class="st">"What is p-hacking?"</span>,</span>
<span>  system <span class="op">=</span> <span class="st">"You are a statistics professor. Explain concepts precisely</span></span>
<span><span class="st">            but accessibly, using real-world examples."</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Constrain output format</span></span>
<span><span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span></span>
<span>  <span class="st">"List three advantages of version control"</span>,</span>
<span>  system <span class="op">=</span> <span class="st">"Respond in bullet points. Be concise -- no more than one sentence per point."</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set a persona</span></span>
<span><span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span></span>
<span>  <span class="st">"How should I structure a data analysis project?"</span>,</span>
<span>  system <span class="op">=</span> <span class="st">"You are a senior R developer who follows tidyverse conventions</span></span>
<span><span class="st">            and emphasizes reproducibility."</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="controlling-generation-parameters">Controlling Generation Parameters<a class="anchor" aria-label="anchor" href="#controlling-generation-parameters"></a>
</h3>
<p>Two parameters give you direct control over the model’s output:</p>
<ul>
<li>
<strong><code>max_tokens</code></strong>: The maximum number of
tokens in the response. Increase for detailed answers, decrease for
concise ones.</li>
<li>
<strong><code>temperature</code></strong>: Controls randomness.
Values near 0 produce deterministic, focused output. Values near 2
produce more creative, varied responses.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Short, focused answer</span></span>
<span><span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span></span>
<span>  <span class="st">"Define overfitting in one sentence."</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">50</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0.1</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Longer, more creative response</span></span>
<span><span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span></span>
<span>  <span class="st">"Write a haiku about data science."</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">1.5</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="multi-turn-conversations">Multi-Turn Conversations<a class="anchor" aria-label="anchor" href="#multi-turn-conversations"></a>
</h2>
<div class="section level3">
<h3 id="creating-a-conversation">Creating a Conversation<a class="anchor" aria-label="anchor" href="#creating-a-conversation"></a>
</h3>
<p><code><a href="../reference/hf_conversation.html">hf_conversation()</a></code> creates a persistent conversation
object that maintains message history across turns. Each call to
<code><a href="../reference/chat.html">chat()</a></code> appends the new exchange and sends the full history
to the model, enabling context-aware responses.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">convo</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hf_conversation.html">hf_conversation</a></span><span class="op">(</span></span>
<span>  system <span class="op">=</span> <span class="st">"You are a helpful R programming tutor. Give concise answers with</span></span>
<span><span class="st">            code examples when appropriate."</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="adding-messages">Adding Messages<a class="anchor" aria-label="anchor" href="#adding-messages"></a>
</h3>
<p>Use the <code><a href="../reference/chat.html">chat()</a></code> generic to add user messages and receive
responses.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">convo</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">convo</span>, <span class="st">"How do I read a CSV file in R?"</span><span class="op">)</span></span>
<span><span class="co">#&gt; assistant: You can use readr::read_csv() for a fast, tibble-based approach:</span></span>
<span><span class="co">#&gt;   library(readr)</span></span>
<span><span class="co">#&gt;   df &lt;- read_csv("data.csv")</span></span>
<span></span>
<span><span class="va">convo</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">convo</span>, <span class="st">"What if the file uses semicolons as delimiters?"</span><span class="op">)</span></span>
<span><span class="co">#&gt; assistant: Use read_csv2() for semicolon-delimited files, or specify</span></span>
<span><span class="co">#&gt;   the delimiter explicitly with read_delim():</span></span>
<span><span class="co">#&gt;   df &lt;- read_delim("data.csv", delim = ";")</span></span>
<span></span>
<span><span class="va">convo</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">convo</span>, <span class="st">"How do I handle missing values during import?"</span><span class="op">)</span></span>
<span><span class="co">#&gt; assistant: read_csv() automatically converts empty strings and "NA" to</span></span>
<span><span class="co">#&gt;   NA values. For custom missing indicators, use the na argument:</span></span>
<span><span class="co">#&gt;   df &lt;- read_csv("data.csv", na = c("", "NA", "N/A", "-999"))</span></span></code></pre></div>
<p>Notice that the model’s third response builds on the earlier context
about file reading, even though the question alone is ambiguous.</p>
</div>
<div class="section level3">
<h3 id="inspecting-the-conversation">Inspecting the Conversation<a class="anchor" aria-label="anchor" href="#inspecting-the-conversation"></a>
</h3>
<p>Print the conversation object to see the full history:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">convo</span><span class="op">)</span></span>
<span><span class="co">#&gt; HF Conversation (model: HuggingFaceTB/SmolLM3-3B)</span></span>
<span><span class="co">#&gt; System: You are a helpful R programming tutor...</span></span>
<span><span class="co">#&gt; ──────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; User: How do I read a CSV file in R?</span></span>
<span><span class="co">#&gt; Assistant: You can use readr::read_csv()...</span></span>
<span><span class="co">#&gt; ──────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; User: What if the file uses semicolons as delimiters?</span></span>
<span><span class="co">#&gt; Assistant: Use read_csv2()...</span></span>
<span><span class="co">#&gt; ...</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="practical-example-iterative-analysis-assistant">Practical Example: Iterative Analysis Assistant<a class="anchor" aria-label="anchor" href="#practical-example-iterative-analysis-assistant"></a>
</h3>
<p>Conversations are useful for iterative data analysis workflows where
each step depends on prior context.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">analyst</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hf_conversation.html">hf_conversation</a></span><span class="op">(</span></span>
<span>  system <span class="op">=</span> <span class="st">"You are a data analysis assistant. The user has a tibble called</span></span>
<span><span class="st">            'sales' with columns: date, region, product, revenue, quantity.</span></span>
<span><span class="st">            Help them explore and analyze this data using tidyverse functions."</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">analyst</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">analyst</span>, <span class="st">"Show me monthly revenue trends by region"</span><span class="op">)</span></span>
<span><span class="va">analyst</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">analyst</span>, <span class="st">"Now add a 3-month rolling average"</span><span class="op">)</span></span>
<span><span class="va">analyst</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">analyst</span>, <span class="st">"Which region has the highest growth rate?"</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="text-generation-with-hf_generate">Text Generation with hf_generate()<a class="anchor" aria-label="anchor" href="#text-generation-with-hf_generate"></a>
</h2>
<div class="section level3">
<h3 id="prompt-completion">Prompt Completion<a class="anchor" aria-label="anchor" href="#prompt-completion"></a>
</h3>
<p><code><a href="../reference/hf_generate.html">hf_generate()</a></code> takes a text prompt and returns a
continuation. Unlike <code><a href="../reference/hf_chat.html">hf_chat()</a></code>, it does not use a
conversational format – it simply extends the input text.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/hf_generate.html">hf_generate</a></span><span class="op">(</span><span class="st">"The three most important principles of tidy data are"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 1 x 2</span></span>
<span><span class="co">#&gt;   prompt                                             generated_text</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                              &lt;chr&gt;</span></span>
<span><span class="co">#&gt; 1 The three most important principles of tidy dat... Each variable forms a column...</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="controlling-length-and-creativity">Controlling Length and Creativity<a class="anchor" aria-label="anchor" href="#controlling-length-and-creativity"></a>
</h3>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Longer generation</span></span>
<span><span class="fu"><a href="../reference/hf_generate.html">hf_generate</a></span><span class="op">(</span></span>
<span>  <span class="st">"Once upon a time in a small village nestled in the mountains,"</span>,</span>
<span>  max_new_tokens <span class="op">=</span> <span class="fl">200</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0.8</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Deterministic, focused output</span></span>
<span><span class="fu"><a href="../reference/hf_generate.html">hf_generate</a></span><span class="op">(</span></span>
<span>  <span class="st">"The formula for standard deviation is"</span>,</span>
<span>  max_new_tokens <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0.1</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="nucleus-sampling-with-top_p">Nucleus Sampling with top_p<a class="anchor" aria-label="anchor" href="#nucleus-sampling-with-top_p"></a>
</h3>
<p>The <code>top_p</code> parameter (nucleus sampling) restricts
generation to tokens whose cumulative probability exceeds the threshold.
Lower values produce more focused text; higher values allow more
diversity.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Conservative: only consider the most likely tokens</span></span>
<span><span class="fu"><a href="../reference/hf_generate.html">hf_generate</a></span><span class="op">(</span></span>
<span>  <span class="st">"The best way to learn R programming is"</span>,</span>
<span>  top_p <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0.7</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Permissive: consider a wider range of tokens</span></span>
<span><span class="fu"><a href="../reference/hf_generate.html">hf_generate</a></span><span class="op">(</span></span>
<span>  <span class="st">"The best way to learn R programming is"</span>,</span>
<span>  top_p <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0.7</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="batch-generation">Batch Generation<a class="anchor" aria-label="anchor" href="#batch-generation"></a>
</h3>
<p>Pass a character vector to generate completions for multiple prompts
in one call.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prompts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"The advantages of functional programming include"</span>,</span>
<span>  <span class="st">"Reproducible research requires"</span>,</span>
<span>  <span class="st">"The tidyverse philosophy emphasizes"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/hf_generate.html">hf_generate</a></span><span class="op">(</span><span class="va">prompts</span>, max_new_tokens <span class="op">=</span> <span class="fl">60</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="fill-in-the-blank-with-hf_fill_mask">Fill-in-the-Blank with hf_fill_mask()<a class="anchor" aria-label="anchor" href="#fill-in-the-blank-with-hf_fill_mask"></a>
</h2>
<div class="section level3">
<h3 id="basic-usage">Basic Usage<a class="anchor" aria-label="anchor" href="#basic-usage"></a>
</h3>
<p><code><a href="../reference/hf_fill_mask.html">hf_fill_mask()</a></code> uses masked language models (like BERT)
to predict a missing word in context. Replace the target word with
<code>[MASK]</code> and the model returns its top predictions.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/hf_fill_mask.html">hf_fill_mask</a></span><span class="op">(</span><span class="st">"The capital of France is [MASK]."</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 5 x 4</span></span>
<span><span class="co">#&gt;   text                                 token  score filled</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;</span></span>
<span><span class="co">#&gt; 1 The capital of France is [MASK].     paris  0.88  The capital of France is paris.</span></span>
<span><span class="co">#&gt; 2 The capital of France is [MASK].     lyon   0.03  The capital of France is lyon.</span></span>
<span><span class="co">#&gt; 3 The capital of France is [MASK].     lille  0.02  The capital of France is lille.</span></span>
<span><span class="co">#&gt; 4 The capital of France is [MASK].     tours  0.01  The capital of France is tours.</span></span>
<span><span class="co">#&gt; 5 The capital of France is [MASK].     marseille 0.01 The capital of France is marseille.</span></span></code></pre></div>
<p>The <code>filled</code> column shows the complete sentence with each
prediction substituted in place of the mask token.</p>
</div>
<div class="section level3">
<h3 id="controlling-predictions-with-top_k">Controlling Predictions with top_k<a class="anchor" aria-label="anchor" href="#controlling-predictions-with-top_k"></a>
</h3>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Get only the top 3 predictions</span></span>
<span><span class="fu"><a href="../reference/hf_fill_mask.html">hf_fill_mask</a></span><span class="op">(</span><span class="st">"R is a [MASK] for statistical computing."</span>, top_k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="different-mask-tokens">Different Mask Tokens<a class="anchor" aria-label="anchor" href="#different-mask-tokens"></a>
</h3>
<p>BERT-family models use <code>[MASK]</code>, but other architectures
use different tokens. The <code>mask_token</code> parameter lets you
specify the correct token for your model.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># RoBERTa uses &lt;mask&gt; instead of [MASK]</span></span>
<span><span class="fu"><a href="../reference/hf_fill_mask.html">hf_fill_mask</a></span><span class="op">(</span></span>
<span>  <span class="st">"Data science is a &lt;mask&gt; field."</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"FacebookAI/roberta-base"</span>,</span>
<span>  mask_token <span class="op">=</span> <span class="st">"&lt;mask&gt;"</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="use-cases-for-fill-mask">Use Cases for Fill-Mask<a class="anchor" aria-label="anchor" href="#use-cases-for-fill-mask"></a>
</h3>
<p>Fill-mask models are useful beyond simple word prediction:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Explore word associations</span></span>
<span><span class="fu"><a href="../reference/hf_fill_mask.html">hf_fill_mask</a></span><span class="op">(</span><span class="st">"In machine learning, the opposite of overfitting is [MASK]."</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Probe model knowledge</span></span>
<span><span class="fu"><a href="../reference/hf_fill_mask.html">hf_fill_mask</a></span><span class="op">(</span><span class="st">"The R programming language was created by [MASK]."</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Test linguistic expectations</span></span>
<span><span class="fu"><a href="../reference/hf_fill_mask.html">hf_fill_mask</a></span><span class="op">(</span><span class="st">"After the storm, the sky became [MASK]."</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="using-different-models">Using Different Models<a class="anchor" aria-label="anchor" href="#using-different-models"></a>
</h2>
<div class="section level3">
<h3 id="specifying-a-model">Specifying a Model<a class="anchor" aria-label="anchor" href="#specifying-a-model"></a>
</h3>
<p>Any chat-compatible model on the Hub can be used with
<code><a href="../reference/hf_chat.html">hf_chat()</a></code> and <code><a href="../reference/hf_generate.html">hf_generate()</a></code>. For
<code><a href="../reference/hf_fill_mask.html">hf_fill_mask()</a></code>, use any fill-mask model.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Use a larger, more capable model</span></span>
<span><span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span></span>
<span>  <span class="st">"Explain the bias-variance tradeoff"</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"mistralai/Mistral-7B-Instruct-v0.3"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Use a specific provider with the :provider suffix</span></span>
<span><span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span></span>
<span>  <span class="st">"What is tidymodels?"</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"meta-llama/Llama-3-8B-Instruct:together"</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="finding-available-models">Finding Available Models<a class="anchor" aria-label="anchor" href="#finding-available-models"></a>
</h3>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Browse text generation models</span></span>
<span><span class="fu"><a href="../reference/hf_search_models.html">hf_search_models</a></span><span class="op">(</span>task <span class="op">=</span> <span class="st">"text-generation"</span>, sort <span class="op">=</span> <span class="st">"downloads"</span>, limit <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Browse fill-mask models</span></span>
<span><span class="fu"><a href="../reference/hf_search_models.html">hf_search_models</a></span><span class="op">(</span>task <span class="op">=</span> <span class="st">"fill-mask"</span>, sort <span class="op">=</span> <span class="st">"downloads"</span>, limit <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="data-frame-integration">Data Frame Integration<a class="anchor" aria-label="anchor" href="#data-frame-integration"></a>
</h2>
<p>LLM functions can be used within tidyverse pipelines, though keep in
mind that each row triggers an API call.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">products</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span></span>
<span>  name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Ergonomic Keyboard"</span>, <span class="st">"Noise-Canceling Headphones"</span>, <span class="st">"Standing Desk"</span><span class="op">)</span>,</span>
<span>  features <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"split layout, mechanical switches, wrist rest"</span>,</span>
<span>    <span class="st">"40-hour battery, ANC, Bluetooth 5.0"</span>,</span>
<span>    <span class="st">"electric height adjustment, memory presets, cable tray"</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate descriptions for each product</span></span>
<span><span class="va">products</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span></span>
<span>    description <span class="op">=</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map_chr</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">name</span>, <span class="st">"-"</span>, <span class="va">features</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hf_chat.html">hf_chat</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"Write a one-sentence product description for:"</span>, <span class="va">prompt</span><span class="op">)</span>,</span>
<span>        max_tokens <span class="op">=</span> <span class="fl">50</span>,</span>
<span>        temperature <span class="op">=</span> <span class="fl">0.7</span></span>
<span>      <span class="op">)</span></span>
<span>      <span class="va">result</span><span class="op">$</span><span class="va">content</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>    <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="see-also">See Also<a class="anchor" aria-label="anchor" href="#see-also"></a>
</h2>
<ul>
<li>
<a href="getting-started.html">Getting Started</a> – installation
and authentication.</li>
<li>
<a href="hub-datasets-and-modeling.html">Hub Discovery, Datasets,
and Tidymodels Integration</a> – finding LLM models on the Hub.</li>
<li>
<a href="text-classification.html">Text Classification</a> – when
you need structured labels rather than free-form text.</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Alex Farach, Sam Terfa, Jack Penzer.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> | <a href="https://github.com/farach/huggingfaceR" class="external-link">View on GitHub</a></p>
</div>

    </footer>
</div>





  </body>
</html>
