[{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Analyzing the Anthropic Economic Index","text":"Anthropic Economic Index tracks AI integrated real-world economic tasks. Built millions Claude conversations mapped U.S. Department Labor’s O*NET occupational taxonomy, dataset provides granular measures tasks AI augments versus automates, usage varies across geographies, collaboration patterns emerge humans AI. vignette demonstrates use huggingfaceR analyze Anthropic Economic Index perspective AI productivity research. learn : Load dataset directly Hugging Face Hub Apply semantic embeddings occupational task descriptions Discover latent structure AI-affected tasks clustering Measure semantic similarity tasks research concepts Classify tasks along research-relevant dimensions using zero-shot models Visualize embedding space AI-impacted occupations analyses illustrate fundamental difference huggingfaceR conversational LLM interfaces ellmer. ellmer provides R6-based chat interface interacting language models one prompt time, huggingfaceR operates programmatic analytical toolkit: embeds, classifies, clusters, searches across entire corpora reproducible pipelines. workflows replicated conversational prompting alone.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"loading-the-dataset","dir":"Articles","previous_headings":"","what":"Loading the Dataset","title":"Analyzing the Anthropic Economic Index","text":"Anthropic Economic Index hosted file-based repository Hugging Face Hub. data files stored CSVs rather standard Datasets format, load directly via URL. readr::read_csv() function handles transparently. task statements file contains core analytical unit: detailed descriptions workers occupation, drawn O*NET database.","code":"base_url <- paste0(   \"https://huggingface.co/datasets/Anthropic/EconomicIndex/\",   \"resolve/main/release_2025_02_10/\" )  # O*NET task statements with occupational codes task_statements <- read_csv(paste0(base_url, \"onet_task_statements.csv\"))  # Task-level AI usage percentages task_usage <- read_csv(paste0(base_url, \"onet_task_mappings.csv\"))  # Automation vs. augmentation breakdown auto_augment <- read_csv(paste0(base_url, \"automation_vs_augmentation.csv\"))  # Wage and occupation metadata wages <- read_csv(paste0(base_url, \"wage_data.csv\")) task_statements #> # A tibble: 19,530 x 8 #>    `O*NET-SOC Code` Title            `Task ID` Task          `Task Type` #>    <chr>            <chr>                <dbl> <chr>         <chr> #>  1 11-1011.00       Chief Executives      8823 Direct or co~ Core #>  2 11-1011.00       Chief Executives      8831 Appoint depa~ Core #>  3 11-1011.00       Chief Executives      8825 Analyze oper~ Core #>  ..."},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"semantic-embeddings-of-occupational-tasks","dir":"Articles","previous_headings":"","what":"Semantic Embeddings of Occupational Tasks","title":"Analyzing the Anthropic Economic Index","text":"core capability huggingfaceR converting text dense vector representations. embedding O*NET task descriptions, can measure semantic distance pair tasks regardless surface wording. enables similarity search, clustering, dimensionality reduction across full task taxonomy.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"embedding-task-descriptions","dir":"Articles","previous_headings":"Semantic Embeddings of Occupational Tasks","what":"Embedding Task Descriptions","title":"Analyzing the Anthropic Economic Index","text":"Select representative sample tasks compute embeddings. result tibble one row per task, list-column numeric vectors, embedding dimensionality.","code":"# Join task descriptions with their AI usage rates. # The task_mappings file uses lowercase task names, so we normalize case for # the join key. tasks_with_usage <- task_statements |>   select(task_id = `Task ID`, task = Task, title = Title,          soc_code = `O*NET-SOC Code`, task_type = `Task Type`) |>   mutate(task_lower = tolower(task)) |>   inner_join(     task_usage |> mutate(task_lower = tolower(task_name)),     by = \"task_lower\"   ) |>   select(-task_lower, -task_name) |>   rename(ai_usage_pct = pct)  # Sample tasks across the usage distribution for analysis set.seed(42) sample_tasks <- tasks_with_usage |>   mutate(usage_quartile = ntile(ai_usage_pct, 4)) |>   group_by(usage_quartile) |>   slice_sample(n = 50) |>   ungroup()  # Generate embeddings for each task description task_embeddings <- hf_embed(sample_tasks$task) task_embeddings #> # A tibble: 200 x 3 #>    text                                          embedding     n_dims #>    <chr>                                         <list>         <int> #>  1 Direct or coordinate an organization's fin~   <dbl [384]>      384 #>  2 Develop or implement procedures for food s~   <dbl [384]>      384 #>  ..."},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"measuring-task-similarity","dir":"Articles","previous_headings":"Semantic Embeddings of Occupational Tasks","what":"Measuring Task Similarity","title":"Analyzing the Anthropic Economic Index","text":"embeddings hand, can compute pairwise cosine similarity. reveals tasks semantically related, even belong different occupational categories.","code":"# Compare a subset of tasks analytical_tasks <- task_embeddings |>   slice(1:10)  hf_similarity(analytical_tasks) #> # A tibble: 45 x 3 #>    text_1                          text_2                       similarity #>    <chr>                           <chr>                             <dbl> #>  1 Direct or coordinate an org~    Analyze operations to eval~       0.82 #>  2 Direct or coordinate an org~    Develop or implement proce~       0.45 #>  ..."},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"nearest-neighbor-search-for-research-concepts","dir":"Articles","previous_headings":"","what":"Nearest Neighbor Search for Research Concepts","title":"Analyzing the Anthropic Economic Index","text":"AI productivity researchers often want identify occupational tasks closest abstract concepts “creative problem solving” “routine data entry.” hf_nearest_neighbors() function performs semantic search embedded corpus. approach lets researchers map theoretical constructs onto empirical task taxonomy without manual coding. Compare conversational approach ellmer, need prompt LLM 19,000+ task descriptions individually. huggingfaceR processes entire corpus single batch operation.","code":"# Build an embedded document set using the tidytext-style interface task_docs <- sample_tasks |>   select(task, ai_usage_pct, title) |>   hf_embed_text(task)  # Find tasks most similar to \"writing and editing documents\" hf_nearest_neighbors(task_docs, \"writing and editing documents\", k = 5) #> # A tibble: 5 x 5 #>    task                            ai_usage_pct title        embedding  similarity #>    <chr>                                  <dbl> <chr>        <list>          <dbl> #>  1 Write reports, memos, or othe~         0.38  Technical W~ <dbl>           0.91 #>  ...  # Find tasks most similar to \"quantitative data analysis\" hf_nearest_neighbors(task_docs, \"quantitative data analysis\", k = 5)  # Find tasks most similar to \"interpersonal communication\" hf_nearest_neighbors(task_docs, \"interpersonal communication\", k = 5)"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"clustering-tasks-by-semantic-content","dir":"Articles","previous_headings":"","what":"Clustering Tasks by Semantic Content","title":"Analyzing the Anthropic Economic Index","text":"Beyond pairwise comparisons, researchers may want discover latent groupings task space. hf_cluster_texts() function applies k-means clustering embedding vectors identify coherent task families.","code":"# Cluster tasks into semantic groups clustered_tasks <- hf_cluster_texts(task_docs, k = 6)  clustered_tasks |>   group_by(cluster) |>   summarize(     n_tasks = n(),     mean_ai_usage = mean(ai_usage_pct, na.rm = TRUE),     example_task = first(task)   ) |>   arrange(desc(mean_ai_usage)) #> # A tibble: 6 x 4 #>   cluster n_tasks mean_ai_usage example_task #>     <int>   <int>         <dbl> <chr> #> 1       3      28         0.312 Write and edit software code~ #> 2       1      38         0.189 Prepare reports summarizing~ #> ..."},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"extracting-cluster-topics","dir":"Articles","previous_headings":"Clustering Tasks by Semantic Content","what":"Extracting Cluster Topics","title":"Analyzing the Anthropic Economic Index","text":"interpret clusters, hf_extract_topics() identifies representative terms within group. function clusters data internally extracts frequent terms per cluster. unsupervised analysis may reveal tasks high AI usage cluster around writing, analysis, code, low-usage tasks cluster around physical operation, patient care, equipment maintenance.","code":"task_docs |>   hf_extract_topics(text_col = \"task\", k = 6) #> # A tibble: 6 x 2 #>   cluster topic_terms #>     <int> <chr> #> 1       1 prepare, reports, data, financial, ... #> 2       2 develop, programs, training, ... #> ..."},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"zero-shot-classification-of-tasks","dir":"Articles","previous_headings":"","what":"Zero-Shot Classification of Tasks","title":"Analyzing the Anthropic Economic Index","text":"hypothesis-driven research, may want classify tasks along specific dimensions without training supervised model. huggingfaceR’s hf_classify_zero_shot() applies natural language inference model assign labels based textual entailment.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"cognitive-demand-classification","dir":"Articles","previous_headings":"Zero-Shot Classification of Tasks","what":"Cognitive Demand Classification","title":"Analyzing the Anthropic Economic Index","text":"","code":"# Classify a sample of tasks by cognitive demand level cognitive_labels <- c(   \"routine procedural work\",   \"analytical reasoning\",   \"creative problem solving\",   \"interpersonal judgment\" )  high_usage_tasks <- sample_tasks |>   filter(ai_usage_pct > quantile(ai_usage_pct, 0.75)) |>   pull(task)  cognitive_classes <- hf_classify_zero_shot(   high_usage_tasks[1:20],   labels = cognitive_labels )  cognitive_classes |>   group_by(text) |>   slice_max(score, n = 1) |>   ungroup() |>   count(label, sort = TRUE) #> # A tibble: 4 x 2 #>   label                        n #>   <chr>                    <int> #> 1 analytical reasoning        9 #> 2 creative problem solving    6 #> 3 routine procedural work     3 #> 4 interpersonal judgment      2"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"automation-potential-classification","dir":"Articles","previous_headings":"Zero-Shot Classification of Tasks","what":"Automation Potential Classification","title":"Analyzing the Anthropic Economic Index","text":"analysis tests whether zero-shot model’s assessment automation potential correlates observed AI usage patterns real world.","code":"# Classify tasks by automation potential automation_labels <- c(   \"fully automatable by AI\",   \"partially automatable with human oversight\",   \"requires significant human judgment\",   \"cannot be performed by AI\" )  automation_classes <- hf_classify_zero_shot(   sample_tasks$task[1:30],   labels = automation_labels )  # Compare zero-shot predictions with actual AI usage rates automation_summary <- automation_classes |>   group_by(text) |>   slice_max(score, n = 1) |>   ungroup() |>   left_join(     sample_tasks |> select(task, ai_usage_pct),     by = c(\"text\" = \"task\")   )  automation_summary |>   group_by(label) |>   summarize(     n = n(),     mean_actual_usage = mean(ai_usage_pct, na.rm = TRUE),     .groups = \"drop\"   ) |>   arrange(desc(mean_actual_usage))"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"visualizing-the-task-embedding-space","dir":"Articles","previous_headings":"","what":"Visualizing the Task Embedding Space","title":"Analyzing the Anthropic Economic Index","text":"Dimensionality reduction provides visual summary tasks relate semantic space. Since already computed embeddings task_docs, can project 2D using uwot package directly, avoiding redundant API calls. Tasks cluster together projection share semantic content. Color gradients within clusters indicate differential AI adoption among semantically similar tasks, may point factors beyond task content (industry norms tool availability) influence adoption. quick one-visualizations without pre-computed embeddings, can use hf_embed_umap() handles embedding projection single call:","code":"library(uwot)  # Extract the embedding matrix from pre-computed embeddings emb_matrix <- do.call(rbind, task_docs$embedding)  # Project to 2D with UMAP umap_coords <- umap(emb_matrix, n_neighbors = 15, min_dist = 0.1)  # Build plot data plot_data <- task_docs |>   mutate(     umap_1 = umap_coords[, 1],     umap_2 = umap_coords[, 2]   )  ggplot(plot_data, aes(x = umap_1, y = umap_2, color = ai_usage_pct)) +   geom_point(alpha = 0.7, size = 2) +   scale_color_viridis_c(     name = \"AI Usage %\",     labels = scales::percent_format(scale = 100)   ) +   labs(     title = \"Semantic Map of O*NET Tasks by AI Usage\",     subtitle = \"UMAP projection of task embeddings colored by AI adoption rate\",     x = NULL, y = NULL   ) +   theme_minimal() +   theme(     axis.text = element_blank(),     axis.ticks = element_blank(),     panel.grid = element_blank()   ) # Alternative: hf_embed_umap() generates embeddings and projects in one step hf_embed_umap(sample_tasks$task[1:50])"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"visualizing-clusters","dir":"Articles","previous_headings":"Visualizing the Task Embedding Space","what":"Visualizing Clusters","title":"Analyzing the Anthropic Economic Index","text":"","code":"ggplot(plot_data |> left_join(clustered_tasks |> select(task, cluster), by = \"task\"),        aes(x = umap_1, y = umap_2, color = factor(cluster))) +   geom_point(alpha = 0.7, size = 2) +   labs(     title = \"Task Clusters in Embedding Space\",     subtitle = \"K-means clusters projected via UMAP\",     color = \"Cluster\",     x = NULL, y = NULL   ) +   theme_minimal() +   theme(     axis.text = element_blank(),     axis.ticks = element_blank(),     panel.grid = element_blank()   )"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"linking-ai-usage-to-wage-data","dir":"Articles","previous_headings":"","what":"Linking AI Usage to Wage Data","title":"Analyzing the Anthropic Economic Index","text":"AEI dataset includes occupational wage data, enabling researchers examine relationship AI adoption compensation.","code":"# Prepare wage data occupation_wages <- wages |>   filter(MedianSalary > 0, ChanceAuto >= 0) |>   select(soc_code = SOCcode, job_name = JobName, job_family = JobFamily,          median_salary = MedianSalary, chance_auto = ChanceAuto,          job_zone = JobZone)  # Aggregate AI usage by occupation occupation_usage <- tasks_with_usage |>   group_by(soc_code, title) |>   summarize(     mean_ai_usage = mean(ai_usage_pct, na.rm = TRUE),     n_tasks = n(),     .groups = \"drop\"   )  # Join occupation_analysis <- occupation_usage |>   inner_join(occupation_wages, by = \"soc_code\")  ggplot(occupation_analysis, aes(x = median_salary, y = mean_ai_usage)) +   geom_point(alpha = 0.4) +   geom_smooth(method = \"loess\", se = TRUE) +   scale_x_continuous(labels = scales::dollar_format()) +   scale_y_continuous(labels = scales::percent_format(scale = 100)) +   labs(     title = \"AI Usage by Median Salary\",     x = \"Median Annual Salary\",     y = \"Mean AI Usage Rate (across tasks)\"   ) +   theme_minimal()"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"analyzing-collaboration-patterns","dir":"Articles","previous_headings":"","what":"Analyzing Collaboration Patterns","title":"Analyzing the Anthropic Economic Index","text":"AEI categorizes human-AI interactions distinct collaboration patterns: directive (human tells AI produce), feedback loop (iterative refinement), learning (human seeks understanding), task iteration (human builds AI output), validation (human checks AI work). map onto broader distinction automation (directive, feedback loop) augmentation (learning, task iteration, validation). can use zero-shot classification validate whether interaction categories align semantic content tasks associated .","code":"auto_augment #> # A tibble: 6 x 2 #>   interaction_type    pct #>   <chr>             <dbl> #> 1 directive          22.6 #> 2 feedback loop      12.0 #> 3 learning           18.9 #> 4 none                2.9 #> 5 task iteration     25.5 #> 6 validation          2.3 interaction_labels <- c(   \"giving direct instructions\",   \"iterative refinement and feedback\",   \"learning and understanding\",   \"building upon previous output\",   \"checking and validating work\" )  # Classify a sample of task descriptions against interaction patterns interaction_classes <- hf_classify_zero_shot(   sample_tasks$task[1:20],   labels = interaction_labels,   multi_label = TRUE )"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"geographic-analysis-with-the-v3-release","dir":"Articles","previous_headings":"","what":"Geographic Analysis with the v3 Release","title":"Analyzing the Anthropic Economic Index","text":"September 2025 release adds geographic breakdowns AI usage. can load enriched data explore AI adoption varies across countries U.S. states.","code":"v3_url <- paste0(   \"https://huggingface.co/datasets/Anthropic/EconomicIndex/\",   \"resolve/main/release_2025_09_15/data/output/\" )  # Load enriched Claude.ai geographic data geo_data <- read_csv(   paste0(v3_url, \"aei_enriched_claude_ai_2025-08-04_to_2025-08-11.csv\") )  # Filter to country-level usage metrics country_usage <- geo_data |>   filter(     geography == \"country\",     facet == \"onet_task\",     variable == \"onet_task_pct\",     level == 0   ) |>   select(geo_id, cluster_name, value)  # Identify top tasks per country top_tasks_by_country <- country_usage |>   group_by(geo_id) |>   slice_max(value, n = 5) |>   ungroup()"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"embedding-geographic-task-profiles","dir":"Articles","previous_headings":"Geographic Analysis with the v3 Release","what":"Embedding Geographic Task Profiles","title":"Analyzing the Anthropic Economic Index","text":"country distribution AI usage across tasks. can characterize national AI strategies embedding top tasks country computing inter-country similarity.","code":"# Get unique tasks across top country profiles unique_geo_tasks <- top_tasks_by_country |>   distinct(cluster_name) |>   pull(cluster_name)  geo_task_embeddings <- hf_embed(unique_geo_tasks)  # For each country, compute a weighted average embedding # representing its AI usage profile country_profiles <- top_tasks_by_country |>   left_join(     geo_task_embeddings |> select(text, embedding),     by = c(\"cluster_name\" = \"text\")   )"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"comparison-with-conversational-approaches","dir":"Articles","previous_headings":"","what":"Comparison with Conversational Approaches","title":"Analyzing the Anthropic Economic Index","text":"analyses illustrate capabilities distinguish huggingfaceR conversational LLM packages like ellmer. Consider research question: “occupational tasks semantically similar creative writing, AI usage compare?”","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"the-huggingfacer-approach-programmatic-reproducible","dir":"Articles","previous_headings":"Comparison with Conversational Approaches","what":"The huggingfaceR approach (programmatic, reproducible)","title":"Analyzing the Anthropic Economic Index","text":"","code":"# Embed all 19,000+ task descriptions in batch all_embeddings <- tasks_with_usage |>   hf_embed_text(task)  # Find the 20 nearest neighbors to \"creative writing\" creative_tasks <- hf_nearest_neighbors(all_embeddings, \"creative writing\", k = 20)  # Analyze their usage distribution creative_tasks |>   summarize(     mean_usage = mean(ai_usage_pct),     median_usage = median(ai_usage_pct),     sd_usage = sd(ai_usage_pct)   )"},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"the-conversational-approach-ellmer-or-similar","dir":"Articles","previous_headings":"Comparison with Conversational Approaches","what":"The conversational approach (ellmer or similar)","title":"Analyzing the Anthropic Economic Index","text":"chat-based interface, analysis require: Manually prompting LLM task description assess similarity (19,000+ API calls unstructured text responses) Parsing natural language responses numeric similarity scores Handling rate limits, inconsistent outputs, non-deterministic responses guarantee reproducibility across runs huggingfaceR’s embedding-based approach deterministic, operates batch, produces structured numeric output suitable downstream statistical analysis. entire pipeline runs single reproducible script.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"research-applications","dir":"Articles","previous_headings":"","what":"Research Applications","title":"Analyzing the Anthropic Economic Index","text":"combination Anthropic Economic Index huggingfaceR’s analytical tools supports several research directions: Task-level adoption modeling. Use embeddings features regression models predicting AI usage rates, controlling occupation, wages, task characteristics. Semantic distance diffusion. Measure whether AI adoption spreads semantically adjacent tasks time, using longitudinal AEI releases. Skill taxonomy validation. Test whether unsupervised clusters embeddings align established occupational classification systems (SOC major groups). Cross-national specialization. Compare embedding centroids top tasks across countries characterize national AI usage profiles. Automation boundary detection. Use zero-shot classification similarity search identify semantic frontier automatable non-automatable tasks.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Analyzing the Anthropic Economic Index","text":"vignette demonstrated huggingfaceR enables programmatic analysis Anthropic Economic Index: operations run reproducible batch pipelines structured data, producing tibbles suitable statistical modeling visualization. analytical approach complements conversational tools enabling kind corpus-scale, quantitative research individual prompts support.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/anthropic-economic-index.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Analyzing the Anthropic Economic Index","text":"Getting Started – installation authentication. Embeddings, Similarity, Semantic Search – detailed coverage embedding functions. Text Classification – zero-shot classification techniques. Hub Discovery, Datasets, Tidymodels – searching Hub building ML pipelines. Anthropic Economic Index – official reports methodology.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Embeddings, Similarity, and Semantic Search","text":"Text embeddings dense numeric vectors represent semantic content text. Sentences similar meanings produce vectors close together high-dimensional space, even use different words. property makes embeddings useful similarity search, clustering, topic modeling, features machine learning. huggingfaceR provides complete embedding workflow: generate vectors hf_embed(), measure similarity hf_similarity(), search hf_nearest_neighbors(), cluster hf_cluster_texts(), extract topics hf_extract_topics(), visualize hf_embed_umap().","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"basic-usage","dir":"Articles","previous_headings":"Generating Embeddings with hf_embed()","what":"Basic Usage","title":"Embeddings, Similarity, and Semantic Search","text":"hf_embed() accepts character vector returns tibble three columns: text, embedding (list-column numeric vectors), n_dims (dimensionality vector).","code":"sentences <- c(   \"Machine learning is transforming healthcare\",   \"Deep learning models require large datasets\",   \"The weather forecast predicts rain tomorrow\",   \"Clinical trials use statistical methods\",   \"It will be sunny next week\" )  embeddings <- hf_embed(sentences) embeddings #> # A tibble: 5 x 3 #>   text                                        embedding   n_dims #>   <chr>                                       <list>       <int> #> 1 Machine learning is transforming healthcare <dbl [384]>    384 #> 2 Deep learning models require large datasets <dbl [384]>    384 #> 3 The weather forecast predicts rain tomorrow <dbl [384]>    384 #> 4 Clinical trials use statistical methods     <dbl [384]>    384 #> 5 It will be sunny next week                  <dbl [384]>    384"},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"accessing-the-vectors","dir":"Articles","previous_headings":"Generating Embeddings with hf_embed()","what":"Accessing the Vectors","title":"Embeddings, Similarity, and Semantic Search","text":"embedding stored numeric vector inside list-column. can extract individual vectors convert entire set matrix.","code":"# Single vector embeddings$embedding[[1]] #> [1] -0.0234  0.0451  0.0123 ...  # Convert to a matrix (rows = texts, columns = dimensions) emb_matrix <- do.call(rbind, embeddings$embedding) dim(emb_matrix) #> [1]   5 384"},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"choosing-a-model","dir":"Articles","previous_headings":"Generating Embeddings with hf_embed()","what":"Choosing a Model","title":"Embeddings, Similarity, and Semantic Search","text":"default model (BAAI/bge-small-en-v1.5) produces 384-dimensional embeddings offers good balance speed quality English text. can specify feature-extraction model Hub.","code":"# Use a different embedding model embeddings_alt <- hf_embed(   sentences,   model = \"BAAI/bge-base-en-v1.5\"  # 768-dimensional, higher quality )"},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"pairwise-similarity-with-hf_similarity","dir":"Articles","previous_headings":"","what":"Pairwise Similarity with hf_similarity()","title":"Embeddings, Similarity, and Semantic Search","text":"hf_similarity() computes cosine similarity pairs embeddings. Cosine similarity ranges -1 (opposite) 1 (identical), values near 0 indicating semantic relationship. Texts related topics (ML healthcare, ML statistics) score highly, unrelated pairs (ML weather) score near zero. foundation semantic search document deduplication.","code":"hf_similarity(embeddings) #> # A tibble: 10 x 3 #>    text_1                                       text_2                      similarity #>    <chr>                                        <chr>                            <dbl> #>  1 Machine learning is transforming healthcare  Deep learning models ...         0.82 #>  2 Machine learning is transforming healthcare  The weather forecast ...         0.15 #>  3 Machine learning is transforming healthcare  Clinical trials use ...          0.61 #>  4 Machine learning is transforming healthcare  It will be sunny ...             0.12 #>  5 Deep learning models require large datasets  The weather forecast ...         0.11 #> ..."},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"tidytext-integration-hf_embed_text","dir":"Articles","previous_headings":"","what":"Tidytext Integration: hf_embed_text()","title":"Embeddings, Similarity, and Semantic Search","text":"data frame workflows, hf_embed_text() adds embeddings directly existing tibble. recommended entry point already structured data.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"embedding-a-data-frame","dir":"Articles","previous_headings":"Tidytext Integration: hf_embed_text()","what":"Embedding a Data Frame","title":"Embeddings, Similarity, and Semantic Search","text":"result retains original columns adds embedding n_dims.","code":"docs <- tibble(   doc_id = 1:6,   category = c(\"tech\", \"tech\", \"food\", \"food\", \"travel\", \"travel\"),   text = c(     \"Neural networks power modern AI systems\",     \"Cloud computing enables scalable applications\",     \"Fresh pasta requires only flour and eggs\",     \"Sourdough bread needs a mature starter\",     \"Tokyo offers incredible street food and temples\",     \"The Swiss Alps provide world-class hiking trails\"   ) )  docs_embedded <- docs |>   hf_embed_text(text)"},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"semantic-search-with-hf_nearest_neighbors","dir":"Articles","previous_headings":"Tidytext Integration: hf_embed_text()","what":"Semantic Search with hf_nearest_neighbors()","title":"Embeddings, Similarity, and Semantic Search","text":"Find documents similar query string. function embeds query, computes cosine similarity documents, returns top matches. pattern useful FAQ matching, document retrieval, recommendation systems.","code":"docs_embedded |>   hf_nearest_neighbors(\"artificial intelligence applications\", k = 3) #> # A tibble: 3 x 5 #>   doc_id category text                                      similarity #>    <int> <chr>    <chr>                                          <dbl> #> 1      1 tech     Neural networks power modern AI systems        0.78 #> 2      2 tech     Cloud computing enables scalable applic...     0.52 #> 3      5 travel   Tokyo offers incredible street food ...        0.18"},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"clustering-with-hf_cluster_texts","dir":"Articles","previous_headings":"","what":"Clustering with hf_cluster_texts()","title":"Embeddings, Similarity, and Semantic Search","text":"hf_cluster_texts() performs k-means clustering embedding vectors, grouping semantically similar texts together. data must already embedding column (hf_embed_text() hf_embed()). resulting cluster column assigns text group. Texts similar topics assigned cluster.","code":"articles <- tibble(   id = 1:12,   text = c(     # Technology cluster     \"New AI chip doubles processing speed\",     \"Quantum computing reaches error correction milestone\",     \"Open-source language model rivals proprietary alternatives\",     \"Cybersecurity threats increase with IoT adoption\",     # Health cluster     \"Mediterranean diet linked to reduced heart disease risk\",     \"New gene therapy shows promise for rare blood disorders\",     \"Sleep quality affects cognitive performance in older adults\",     \"Vaccine development accelerates with mRNA technology\",     # Environment cluster     \"Arctic ice loss accelerates beyond model predictions\",     \"Renewable energy capacity surpasses coal globally\",     \"Ocean acidification threatens coral reef ecosystems\",     \"Urban forests reduce city temperatures by up to 5 degrees\"   ) )  clustered <- articles |>   hf_embed_text(text) |>   hf_cluster_texts(k = 3)  clustered |>   select(id, text, cluster) |>   arrange(cluster)"},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"topic-extraction-with-hf_extract_topics","dir":"Articles","previous_headings":"","what":"Topic Extraction with hf_extract_topics()","title":"Embeddings, Similarity, and Semantic Search","text":"hf_extract_topics() builds clustering extracting representative keywords cluster. requires tidytext package tokenization TF-IDF computation. cluster described words distinctive documents, making straightforward assign human-readable topic labels.","code":"library(tidytext)  topics <- articles |>   hf_embed_text(text) |>   hf_extract_topics(text_col = \"text\", k = 3, top_n = 5)  topics #> # A tibble: 15 x 3 #>    cluster word           tf_idf #>      <int> <chr>           <dbl> #>  1       1 ai             0.231 #>  2       1 computing      0.198 #>  3       1 chip           0.165 #> ..."},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"visualization-with-hf_embed_umap","dir":"Articles","previous_headings":"","what":"Visualization with hf_embed_umap()","title":"Embeddings, Similarity, and Semantic Search","text":"hf_embed_umap() reduces high-dimensional embeddings 2D coordinates using UMAP (Uniform Manifold Approximation Projection), suitable scatter plot visualization. function requires uwot package. Semantically related texts cluster together 2D projection. n_neighbors min_dist parameters control trade-preserving local versus global structure.","code":"library(ggplot2)  texts <- c(   # Animals   \"cats are independent pets\", \"dogs are loyal companions\",   \"goldfish are low-maintenance pets\", \"parrots can mimic speech\",   # Vehicles    \"sedans are practical family cars\", \"trucks haul heavy loads\",   \"bicycles reduce carbon emissions\", \"motorcycles offer speed and freedom\",   # Food   \"pizza is a popular dinner choice\", \"sushi requires fresh fish\",   \"tacos feature various fillings\", \"pasta comes in many shapes\" )  coords <- hf_embed_umap(texts, n_neighbors = 4, min_dist = 0.1)  ggplot(coords, aes(umap_1, umap_2, label = text)) +   geom_point(size = 2) +   geom_text(hjust = 0, nudge_x = 0.02, size = 3) +   theme_minimal() +   labs(title = \"UMAP Projection of Text Embeddings\",        x = \"UMAP 1\", y = \"UMAP 2\")"},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"practical-example-document-similarity-analysis","dir":"Articles","previous_headings":"","what":"Practical Example: Document Similarity Analysis","title":"Embeddings, Similarity, and Semantic Search","text":"end--end example combines embedding, clustering, visualization analyze set research paper abstracts. embedding model captures domain-specific semantics, automatically discovered clusters align true research fields.","code":"library(ggplot2)  # Simulate research abstracts from three fields abstracts <- tibble(   paper_id = 1:15,   field = rep(c(\"NLP\", \"genomics\", \"climate\"), each = 5),   abstract = c(     \"Transformer architectures improve machine translation quality\",     \"Attention mechanisms capture long-range text dependencies\",     \"Pre-training on large corpora enables few-shot learning\",     \"Named entity recognition benefits from contextual embeddings\",     \"Sentiment analysis models generalize across domains\",     \"CRISPR enables precise genome editing in mammalian cells\",     \"Single-cell RNA sequencing reveals cell type heterogeneity\",     \"Epigenetic modifications regulate gene expression patterns\",     \"Protein folding prediction reaches experimental accuracy\",     \"Microbiome composition correlates with metabolic health\",     \"Global temperatures rise faster than model projections\",     \"Carbon capture technology scales to industrial levels\",     \"Sea level rise threatens coastal infrastructure worldwide\",     \"Deforestation reduces regional precipitation patterns\",     \"Methane emissions from permafrost accelerate warming\"   ) )  # Embed, cluster, and project to 2D result <- abstracts |>   hf_embed_text(abstract) |>   hf_cluster_texts(k = 3)  coords <- hf_embed_umap(abstracts$abstract, n_neighbors = 5)  # Combine for plotting plot_data <- bind_cols(   result |> select(paper_id, field, cluster),   coords |> select(umap_1, umap_2) )  ggplot(plot_data, aes(umap_1, umap_2, color = factor(cluster), shape = field)) +   geom_point(size = 3) +   theme_minimal() +   labs(     title = \"Research Abstracts by Embedding Cluster\",     color = \"Cluster\",     shape = \"True Field\"   )"},{"path":"https://farach.github.io/huggingfaceR/articles/embeddings-and-similarity.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Embeddings, Similarity, and Semantic Search","text":"Getting Started – installation authentication. Hub Discovery, Datasets, Tidymodels Integration – using embeddings features supervised learning pipelines.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"what-is-huggingfacer","dir":"Articles","previous_headings":"","what":"What is huggingfaceR?","title":"Getting Started with huggingfaceR","text":"huggingfaceR provides R users direct access 500,000 machine learning models hosted Hugging Face Hub. package uses Hugging Face Inference API, can perform natural language processing tasks – classification, embeddings, chat, text generation, – without installing Python managing model weights locally. Key design principles: Python required. Authentication network connection need. Tidyverse-native. Every function accepts character vectors returns tibbles. Pipe-friendly. Functions compose naturally dplyr, tidyr, rest tidyverse.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting Started with huggingfaceR","text":"Install released version CRAN development version GitHub:","code":"# From CRAN install.packages(\"huggingfaceR\")  # Development version # install.packages(\"devtools\") devtools::install_github(\"farach/huggingfaceR\")"},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"authentication","dir":"Articles","previous_headings":"","what":"Authentication","title":"Getting Started with huggingfaceR","text":"Hugging Face requires API token inference requests. obtain one: Create free account huggingface.co. Navigate Settings > Access Tokens. Generate token least read access. configure token R: storing token, loaded automatically future sessions.","code":"library(huggingfaceR)  # Store your token persistently (writes to .Renviron) hf_set_token(\"hf_your_token_here\", store = TRUE)  # Verify authentication hf_whoami()"},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"classify-text","dir":"Articles","previous_headings":"Quick Tour","what":"Classify Text","title":"Getting Started with huggingfaceR","text":"Assign labels text using pre-trained classifiers. default model performs sentiment analysis, can supply classification model Hub.","code":"# Sentiment analysis hf_classify(\"I love using R for data science!\")  # Zero-shot classification with custom labels (no training needed) hf_classify_zero_shot(   \"NASA launches new Mars rover\",   labels = c(\"science\", \"politics\", \"sports\", \"entertainment\") )"},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"generate-embeddings","dir":"Articles","previous_headings":"Quick Tour","what":"Generate Embeddings","title":"Getting Started with huggingfaceR","text":"Convert text dense numeric vectors capture semantic meaning. Similar texts produce similar vectors.","code":"sentences <- c(   \"The cat sat on the mat\",   \"A feline rested on the rug\",   \"The dog played in the park\" )  embeddings <- hf_embed(sentences) embeddings  # Compute pairwise cosine similarity hf_similarity(embeddings)"},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"chat-with-a-language-model","dir":"Articles","previous_headings":"Quick Tour","what":"Chat with a Language Model","title":"Getting Started with huggingfaceR","text":"Interact open-source large language models simple interface.","code":"# Single question hf_chat(\"What is the tidyverse?\")  # Guide the model with a system prompt hf_chat(   \"Explain logistic regression in two sentences.\",   system = \"You are a statistics instructor. Use plain language.\" )"},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"explore-the-hub","dir":"Articles","previous_headings":"Quick Tour","what":"Explore the Hub","title":"Getting Started with huggingfaceR","text":"Search models load datasets directly R without leaving session.","code":"# Find popular text classification models hf_search_models(task = \"text-classification\", limit = 5)  # Load dataset rows into a tibble imdb <- hf_load_dataset(\"imdb\", split = \"train\", limit = 100) head(imdb)"},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"working-with-data-frames","dir":"Articles","previous_headings":"","what":"Working with Data Frames","title":"Getting Started with huggingfaceR","text":"huggingfaceR functions accept character vectors return tibbles, integrate naturally tidyverse pipelines.","code":"library(dplyr) library(tidyr)  reviews <- tibble(    product_id = 1:5,   review = c(     \"Excellent quality, highly recommend!\",     \"Broke after one week of use\",     \"Good value for the price\",     \"Disappointing, not as advertised\",     \"Love it! Will buy again\"   ) )  # Add sentiment scores reviews |>   mutate(sentiment = hf_classify(review)) |>   unnest(sentiment) |>   select(product_id, review, label, score)"},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Getting Started with huggingfaceR","text":"deeper coverage capability, see following vignettes: Text Classification Zero-Shot Labeling – sentiment analysis, custom categories, data frame workflows. Embeddings, Similarity, Semantic Search – vector representations, clustering, topic modeling, visualization. Chat, Conversations, Text Generation – LLM interaction patterns, multi-turn conversations, fill-mask. Hub Discovery, Datasets, Tidymodels Integration – searching models, loading data, building ML pipelines embeddings. Analyzing Anthropic Economic Index – research-oriented case study using embeddings, clustering, zero-shot classification real-world AI adoption data.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/getting-started.html","id":"additional-resources","dir":"Articles","previous_headings":"","what":"Additional Resources","title":"Getting Started with huggingfaceR","text":"Hugging Face Inference API documentation Hugging Face Model Hub Hugging Face Datasets Report issues GitHub","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"Hugging Face Hub hosts 500,000 models 100,000 datasets. huggingfaceR provides functions search registry, load data directly tibbles, integrate embeddings tidymodels machine learning workflows – without leaving R.","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"by-task","dir":"Articles","previous_headings":"Searching Models","what":"By Task","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"hf_search_models() queries Hub returns tibble matching models. task parameter filters pipeline type.","code":"# Text classification models hf_search_models(task = \"text-classification\", limit = 10) #> # A tibble: 10 x 7 #>    model_id                              author    task                downloads  likes tags     library #>    <chr>                                 <chr>     <chr>                   <int>  <int> <list>   <chr> #>  1 distilbert/distilbert-base-uncased... distilbert text-classification  5234891   412 <chr>    transformers #>  2 ...  # Embedding models hf_search_models(task = \"feature-extraction\", limit = 5)  # Text generation models hf_search_models(task = \"text-generation\", limit = 5)"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"by-author-or-search-term","dir":"Articles","previous_headings":"Searching Models","what":"By Author or Search Term","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"","code":"# Models from a specific organization hf_search_models(author = \"facebook\", limit = 10)  # Free-text search hf_search_models(search = \"sentiment english\", limit = 5)  # Combine filters hf_search_models(   task = \"text-classification\",   search = \"emotion\",   sort = \"likes\",   limit = 5 )"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"sorting-and-pagination","dir":"Articles","previous_headings":"Searching Models","what":"Sorting and Pagination","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"Results can sorted \"downloads\" \"likes\".","code":"# Most downloaded fill-mask models hf_search_models(   task = \"fill-mask\",   sort = \"downloads\",   limit = 10 )  # Most liked text-generation models hf_search_models(   task = \"text-generation\",   sort = \"likes\",   limit = 10 )"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"from-discovery-to-usage","dir":"Articles","previous_headings":"Searching Models","what":"From Discovery to Usage","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"Model IDs returned hf_search_models() can passed directly inference functions.","code":"# Find a model models <- hf_search_models(task = \"text-classification\", search = \"emotion\")  # Use it for classification hf_classify(   \"I'm so happy today!\",   model = models$model_id[1] )"},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"inspecting-a-specific-model","dir":"Articles","previous_headings":"Model Details","what":"Inspecting a Specific Model","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"hf_model_info() returns detailed metadata single model, including tags, library, pipeline type, download statistics.","code":"info <- hf_model_info(\"BAAI/bge-small-en-v1.5\")  # Key fields info$pipeline_tag #> [1] \"feature-extraction\"  info$downloads #> [1] 12345678  info$tags #> [1] \"feature-extraction\" \"embeddings\" \"sentence-similarity\" ..."},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"listing-available-tasks","dir":"Articles","previous_headings":"Model Details","what":"Listing Available Tasks","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"hf_list_tasks() returns task types recognized Hub. Use pattern parameter filter regex.","code":"# All tasks hf_list_tasks()  # Only classification-related tasks hf_list_tasks(pattern = \"classification\") #> [1] \"text-classification\"      \"token-classification\" #> [3] \"zero-shot-classification\" \"image-classification\" #> [5] \"audio-classification\""},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"searching-datasets","dir":"Articles","previous_headings":"","what":"Searching Datasets","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"hf_search_datasets() queries Hub’s dataset registry. interface mirrors hf_search_models().","code":"# Find sentiment datasets hf_search_datasets(search = \"sentiment\", limit = 5) #> # A tibble: 5 x 5 #>   dataset_id               author    downloads  likes tags #>   <chr>                    <chr>         <int>  <int> <list>  # Filter by task hf_search_datasets(task = \"text-classification\", limit = 10)  # Sort by popularity hf_search_datasets(search = \"translation\", sort = \"likes\", limit = 5)"},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"basic-usage","dir":"Articles","previous_headings":"Loading Datasets with hf_load_dataset()","what":"Basic Usage","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"hf_load_dataset() fetches dataset rows Hub’s Datasets Server API returns tibble. Python local downloads required. function automatically resolves short dataset names (e.g., \"imdb\" becomes \"stanfordnlp/imdb\") detects appropriate configuration.","code":"imdb <- hf_load_dataset(\"imdb\", split = \"train\", limit = 100) imdb #> # A tibble: 100 x 4 #>    text                                                    label .dataset .split #>    <chr>                                                   <int> <chr>    <chr> #>  1 I rented I AM CURIOUS-YELLOW from my video store be...      0 stanfo...  train #>  2 \"\\\"I Am Curious: Yellow\\\" is a risque a]nd target...        0 stanfo...  train #> ..."},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"configs-and-splits","dir":"Articles","previous_headings":"Loading Datasets with hf_load_dataset()","what":"Configs and Splits","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"datasets multiple configurations (subsets). config parameter lets specify one load. omitted, default config auto-detected.","code":"# Explicitly specify a config hf_load_dataset(\"stanfordnlp/imdb\", split = \"test\", config = \"plain_text\", limit = 50)  # Different splits train <- hf_load_dataset(\"imdb\", split = \"train\", limit = 500) test <- hf_load_dataset(\"imdb\", split = \"test\", limit = 500)"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"pagination-for-large-datasets","dir":"Articles","previous_headings":"Loading Datasets with hf_load_dataset()","what":"Pagination for Large Datasets","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"Use offset paginate large datasets batches.","code":"# First 1000 rows batch1 <- hf_load_dataset(\"imdb\", split = \"train\", limit = 1000, offset = 0)  # Next 1000 rows batch2 <- hf_load_dataset(\"imdb\", split = \"train\", limit = 1000, offset = 1000)  # Combine full_data <- bind_rows(batch1, batch2)"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"dataset-metadata","dir":"Articles","previous_headings":"Loading Datasets with hf_load_dataset()","what":"Dataset Metadata","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"hf_dataset_info() returns metadata dataset without downloading rows.","code":"info <- hf_dataset_info(\"imdb\") names(info)"},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"the-recipe-step","dir":"Articles","previous_headings":"Tidymodels Integration with step_hf_embed()","what":"The Recipe Step","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"step_hf_embed() tidymodels recipe step converts text columns embedding features prep()/bake() workflow. text column replaced numeric columns named {column}_emb_1, {column}_emb_2, …, {column}_emb_384 (one per embedding dimension).","code":"library(tidymodels)  # Sample data train_data <- tibble(   text = c(     \"This movie was fantastic, truly moving\",     \"Terrible acting and boring plot\",     \"A masterpiece of modern cinema\",     \"Waste of time, do not watch\",     \"Beautiful story and great performances\",     \"Dull and predictable from start to finish\"   ),   sentiment = factor(c(\"pos\", \"neg\", \"pos\", \"neg\", \"pos\", \"neg\")) )  # Define a recipe with embedding features rec <- recipe(sentiment ~ text, data = train_data) |>   step_hf_embed(text)  # Prep computes column metadata rec_prepped <- prep(rec)  # Bake generates the actual embeddings baked <- bake(rec_prepped, new_data = train_data) names(baked)[1:5] #> [1] \"text_emb_1\" \"text_emb_2\" \"text_emb_3\" \"text_emb_4\" \"text_emb_5\" dim(baked) #> [1]   6 385  # 384 embedding dims + 1 outcome column"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"complete-classification-workflow","dir":"Articles","previous_headings":"Tidymodels Integration with step_hf_embed()","what":"Complete Classification Workflow","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"example builds full supervised learning pipeline: load data Hub, create embeddings recipe, train model, evaluate predictions.","code":"library(tidymodels)  # Load labeled data imdb_train <- hf_load_dataset(\"imdb\", split = \"train\", limit = 200) |>   mutate(sentiment = factor(ifelse(label == 1, \"pos\", \"neg\"))) |>   select(text, sentiment)  imdb_test <- hf_load_dataset(\"imdb\", split = \"test\", limit = 50) |>   mutate(sentiment = factor(ifelse(label == 1, \"pos\", \"neg\"))) |>   select(text, sentiment)  # Define recipe embedding_recipe <- recipe(sentiment ~ text, data = imdb_train) |>   step_hf_embed(text)  # Define model lr_model <- logistic_reg() |>   set_engine(\"glm\")  # Build workflow wf <- workflow() |>   add_recipe(embedding_recipe) |>   add_model(lr_model)  # Train fitted_wf <- fit(wf, data = imdb_train)  # Predict on test set predictions <- predict(fitted_wf, new_data = imdb_test) |>   bind_cols(imdb_test)  # Evaluate predictions |>   metrics(truth = sentiment, estimate = .pred_class)"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"using-a-different-embedding-model","dir":"Articles","previous_headings":"Tidymodels Integration with step_hf_embed()","what":"Using a Different Embedding Model","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"model parameter step_hf_embed() specifies embedding model use. Different models may produce different feature quality depending domain.","code":"# Higher-dimensional embeddings recipe(sentiment ~ text, data = train_data) |>   step_hf_embed(text, model = \"BAAI/bge-base-en-v1.5\")  # 768 dims"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"inspecting-and-tuning-the-step","dir":"Articles","previous_headings":"Tidymodels Integration with step_hf_embed()","what":"Inspecting and Tuning the Step","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"tidy() extracts step’s configuration, tunable() reports parameters support tuning.","code":"# View step configuration tidy(rec_prepped, number = 1)  # Check tunable parameters tunable(rec_prepped$steps[[1]])"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"practical-considerations","dir":"Articles","previous_headings":"Tidymodels Integration with step_hf_embed()","what":"Practical Considerations","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"API rate limits. bake() makes one API call per text row. large datasets, consider pre-computing embeddings hf_embed() saving disk saveRDS(). Caching strategy. Compute embeddings training set prep(), reuse predictions. recipe stores trained step configuration. Model selection. Smaller embedding models (384 dims) train downstream models faster. Larger models (768+ dims) may improve accuracy nuanced tasks. Experiment validation set.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"end-to-end-example-topic-classification-pipeline","dir":"Articles","previous_headings":"","what":"End-to-End Example: Topic Classification Pipeline","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"example combines Hub discovery, dataset loading, modeling complete workflow.","code":"library(tidymodels)  # Step 1: Discover a suitable dataset hf_search_datasets(search = \"news classification\", limit = 5)  # Step 2: Load and prepare data news_train <- hf_load_dataset(\"stanfordnlp/imdb\", split = \"train\", limit = 300) |>   mutate(label = factor(ifelse(label == 1, \"positive\", \"negative\"))) |>   select(text, label)  news_test <- hf_load_dataset(\"stanfordnlp/imdb\", split = \"test\", limit = 100) |>   mutate(label = factor(ifelse(label == 1, \"positive\", \"negative\"))) |>   select(text, label)  # Step 3: Find a good embedding model hf_search_models(task = \"feature-extraction\", search = \"bge\", limit = 5)  # Step 4: Build and train the pipeline wf <- workflow() |>   add_recipe(     recipe(label ~ text, data = news_train) |>       step_hf_embed(text, model = \"BAAI/bge-small-en-v1.5\")   ) |>   add_model(logistic_reg())  fitted <- fit(wf, data = news_train)  # Step 5: Evaluate results <- predict(fitted, news_test) |>   bind_cols(news_test)  results |>   conf_mat(truth = label, estimate = .pred_class)  results |>   metrics(truth = label, estimate = .pred_class)"},{"path":"https://farach.github.io/huggingfaceR/articles/hub-datasets-and-modeling.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Hub Discovery, Datasets, and Tidymodels Integration","text":"Getting Started – installation authentication. Embeddings, Similarity, Semantic Search – understanding embeddings, similarity, unsupervised analysis. Text Classification – direct API-based classification without training model.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Chat, Conversations, and Text Generation","text":"huggingfaceR provides access open-source large language models (LLMs) Hugging Face Inference Providers API. can ask questions, hold multi-turn conversations, generate text continuations, probe masked language models – without downloading model weights managing GPU resources. default model chat generation HuggingFaceTB/SmolLM3-3B, compact yet capable open-source model. can substitute chat-compatible model available Hub.","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"basic-question-answer","dir":"Articles","previous_headings":"Single-Turn Chat with hf_chat()","what":"Basic Question-Answer","title":"Chat, Conversations, and Text Generation","text":"hf_chat() sends single message language model returns response tibble. returned tibble includes model’s response (content), model identifier, number tokens consumed.","code":"hf_chat(\"What are the main differences between R and Python for data analysis?\") #> # A tibble: 1 x 4 #>   role      content                                        model         tokens_used #>   <chr>     <chr>                                          <chr>               <int> #> 1 assistant R and Python are both popular for data anal... HuggingFace...        127"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"system-prompts","dir":"Articles","previous_headings":"Single-Turn Chat with hf_chat()","what":"System Prompts","title":"Chat, Conversations, and Text Generation","text":"System prompts define model’s behavior, personality, domain expertise. sent user message persist duration request.","code":"# Act as a domain expert hf_chat(   \"What is p-hacking?\",   system = \"You are a statistics professor. Explain concepts precisely             but accessibly, using real-world examples.\" )  # Constrain output format hf_chat(   \"List three advantages of version control\",   system = \"Respond in bullet points. Be concise -- no more than one sentence per point.\" )  # Set a persona hf_chat(   \"How should I structure a data analysis project?\",   system = \"You are a senior R developer who follows tidyverse conventions             and emphasizes reproducibility.\" )"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"controlling-generation-parameters","dir":"Articles","previous_headings":"Single-Turn Chat with hf_chat()","what":"Controlling Generation Parameters","title":"Chat, Conversations, and Text Generation","text":"Two parameters give direct control model’s output: max_tokens: maximum number tokens response. Increase detailed answers, decrease concise ones. temperature: Controls randomness. Values near 0 produce deterministic, focused output. Values near 2 produce creative, varied responses.","code":"# Short, focused answer hf_chat(   \"Define overfitting in one sentence.\",   max_tokens = 50,   temperature = 0.1 )  # Longer, more creative response hf_chat(   \"Write a haiku about data science.\",   max_tokens = 100,   temperature = 1.5 )"},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"creating-a-conversation","dir":"Articles","previous_headings":"Multi-Turn Conversations","what":"Creating a Conversation","title":"Chat, Conversations, and Text Generation","text":"hf_conversation() creates persistent conversation object maintains message history across turns. call chat() appends new exchange sends full history model, enabling context-aware responses.","code":"convo <- hf_conversation(   system = \"You are a helpful R programming tutor. Give concise answers with             code examples when appropriate.\" )"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"adding-messages","dir":"Articles","previous_headings":"Multi-Turn Conversations","what":"Adding Messages","title":"Chat, Conversations, and Text Generation","text":"Use chat() generic add user messages receive responses. Notice model’s third response builds earlier context file reading, even though question alone ambiguous.","code":"convo <- chat(convo, \"How do I read a CSV file in R?\") #> assistant: You can use readr::read_csv() for a fast, tibble-based approach: #>   library(readr) #>   df <- read_csv(\"data.csv\")  convo <- chat(convo, \"What if the file uses semicolons as delimiters?\") #> assistant: Use read_csv2() for semicolon-delimited files, or specify #>   the delimiter explicitly with read_delim(): #>   df <- read_delim(\"data.csv\", delim = \";\")  convo <- chat(convo, \"How do I handle missing values during import?\") #> assistant: read_csv() automatically converts empty strings and \"NA\" to #>   NA values. For custom missing indicators, use the na argument: #>   df <- read_csv(\"data.csv\", na = c(\"\", \"NA\", \"N/A\", \"-999\"))"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"inspecting-the-conversation","dir":"Articles","previous_headings":"Multi-Turn Conversations","what":"Inspecting the Conversation","title":"Chat, Conversations, and Text Generation","text":"Print conversation object see full history:","code":"print(convo) #> HF Conversation (model: HuggingFaceTB/SmolLM3-3B) #> System: You are a helpful R programming tutor... #> ────────────────────────────────────────────────── #> User: How do I read a CSV file in R? #> Assistant: You can use readr::read_csv()... #> ────────────────────────────────────────────────── #> User: What if the file uses semicolons as delimiters? #> Assistant: Use read_csv2()... #> ..."},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"practical-example-iterative-analysis-assistant","dir":"Articles","previous_headings":"Multi-Turn Conversations","what":"Practical Example: Iterative Analysis Assistant","title":"Chat, Conversations, and Text Generation","text":"Conversations useful iterative data analysis workflows step depends prior context.","code":"analyst <- hf_conversation(   system = \"You are a data analysis assistant. The user has a tibble called             'sales' with columns: date, region, product, revenue, quantity.             Help them explore and analyze this data using tidyverse functions.\" )  analyst <- chat(analyst, \"Show me monthly revenue trends by region\") analyst <- chat(analyst, \"Now add a 3-month rolling average\") analyst <- chat(analyst, \"Which region has the highest growth rate?\")"},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"prompt-completion","dir":"Articles","previous_headings":"Text Generation with hf_generate()","what":"Prompt Completion","title":"Chat, Conversations, and Text Generation","text":"hf_generate() takes text prompt returns continuation. Unlike hf_chat(), use conversational format – simply extends input text.","code":"hf_generate(\"The three most important principles of tidy data are\") #> # A tibble: 1 x 2 #>   prompt                                             generated_text #>   <chr>                                              <chr> #> 1 The three most important principles of tidy dat... Each variable forms a column..."},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"controlling-length-and-creativity","dir":"Articles","previous_headings":"Text Generation with hf_generate()","what":"Controlling Length and Creativity","title":"Chat, Conversations, and Text Generation","text":"","code":"# Longer generation hf_generate(   \"Once upon a time in a small village nestled in the mountains,\",   max_new_tokens = 200,   temperature = 0.8 )  # Deterministic, focused output hf_generate(   \"The formula for standard deviation is\",   max_new_tokens = 100,   temperature = 0.1 )"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"nucleus-sampling-with-top_p","dir":"Articles","previous_headings":"Text Generation with hf_generate()","what":"Nucleus Sampling with top_p","title":"Chat, Conversations, and Text Generation","text":"top_p parameter (nucleus sampling) restricts generation tokens whose cumulative probability exceeds threshold. Lower values produce focused text; higher values allow diversity.","code":"# Conservative: only consider the most likely tokens hf_generate(   \"The best way to learn R programming is\",   top_p = 0.5,   temperature = 0.7 )  # Permissive: consider a wider range of tokens hf_generate(   \"The best way to learn R programming is\",   top_p = 0.95,   temperature = 0.7 )"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"batch-generation","dir":"Articles","previous_headings":"Text Generation with hf_generate()","what":"Batch Generation","title":"Chat, Conversations, and Text Generation","text":"Pass character vector generate completions multiple prompts one call.","code":"prompts <- c(   \"The advantages of functional programming include\",   \"Reproducible research requires\",   \"The tidyverse philosophy emphasizes\" )  hf_generate(prompts, max_new_tokens = 60)"},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"basic-usage","dir":"Articles","previous_headings":"Fill-in-the-Blank with hf_fill_mask()","what":"Basic Usage","title":"Chat, Conversations, and Text Generation","text":"hf_fill_mask() uses masked language models (like BERT) predict missing word context. Replace target word [MASK] model returns top predictions. filled column shows complete sentence prediction substituted place mask token.","code":"hf_fill_mask(\"The capital of France is [MASK].\") #> # A tibble: 5 x 4 #>   text                                 token  score filled #>   <chr>                                <chr>  <dbl> <chr> #> 1 The capital of France is [MASK].     paris  0.88  The capital of France is paris. #> 2 The capital of France is [MASK].     lyon   0.03  The capital of France is lyon. #> 3 The capital of France is [MASK].     lille  0.02  The capital of France is lille. #> 4 The capital of France is [MASK].     tours  0.01  The capital of France is tours. #> 5 The capital of France is [MASK].     marseille 0.01 The capital of France is marseille."},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"controlling-predictions-with-top_k","dir":"Articles","previous_headings":"Fill-in-the-Blank with hf_fill_mask()","what":"Controlling Predictions with top_k","title":"Chat, Conversations, and Text Generation","text":"","code":"# Get only the top 3 predictions hf_fill_mask(\"R is a [MASK] for statistical computing.\", top_k = 3)"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"different-mask-tokens","dir":"Articles","previous_headings":"Fill-in-the-Blank with hf_fill_mask()","what":"Different Mask Tokens","title":"Chat, Conversations, and Text Generation","text":"BERT-family models use [MASK], architectures use different tokens. mask_token parameter lets specify correct token model.","code":"# RoBERTa uses <mask> instead of [MASK] hf_fill_mask(   \"Data science is a <mask> field.\",   model = \"FacebookAI/roberta-base\",   mask_token = \"<mask>\" )"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"use-cases-for-fill-mask","dir":"Articles","previous_headings":"Fill-in-the-Blank with hf_fill_mask()","what":"Use Cases for Fill-Mask","title":"Chat, Conversations, and Text Generation","text":"Fill-mask models useful beyond simple word prediction:","code":"# Explore word associations hf_fill_mask(\"In machine learning, the opposite of overfitting is [MASK].\")  # Probe model knowledge hf_fill_mask(\"The R programming language was created by [MASK].\")  # Test linguistic expectations hf_fill_mask(\"After the storm, the sky became [MASK].\")"},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"specifying-a-model","dir":"Articles","previous_headings":"Using Different Models","what":"Specifying a Model","title":"Chat, Conversations, and Text Generation","text":"chat-compatible model Hub can used hf_chat() hf_generate(). hf_fill_mask(), use fill-mask model.","code":"# Use a larger, more capable model hf_chat(   \"Explain the bias-variance tradeoff\",   model = \"mistralai/Mistral-7B-Instruct-v0.3\" )  # Use a specific provider with the :provider suffix hf_chat(   \"What is tidymodels?\",   model = \"meta-llama/Llama-3-8B-Instruct:together\" )"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"finding-available-models","dir":"Articles","previous_headings":"Using Different Models","what":"Finding Available Models","title":"Chat, Conversations, and Text Generation","text":"","code":"# Browse text generation models hf_search_models(task = \"text-generation\", sort = \"downloads\", limit = 10)  # Browse fill-mask models hf_search_models(task = \"fill-mask\", sort = \"downloads\", limit = 5)"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"data-frame-integration","dir":"Articles","previous_headings":"","what":"Data Frame Integration","title":"Chat, Conversations, and Text Generation","text":"LLM functions can used within tidyverse pipelines, though keep mind row triggers API call.","code":"library(dplyr)  products <- tibble(   name = c(\"Ergonomic Keyboard\", \"Noise-Canceling Headphones\", \"Standing Desk\"),   features = c(     \"split layout, mechanical switches, wrist rest\",     \"40-hour battery, ANC, Bluetooth 5.0\",     \"electric height adjustment, memory presets, cable tray\"   ) )  # Generate descriptions for each product products |>   mutate(     description = purrr::map_chr(paste(name, \"-\", features), function(prompt) {       result <- hf_chat(         paste(\"Write a one-sentence product description for:\", prompt),         max_tokens = 50,         temperature = 0.7       )       result$content[1]     })   )"},{"path":"https://farach.github.io/huggingfaceR/articles/llm-chat-and-generation.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Chat, Conversations, and Text Generation","text":"Getting Started – installation authentication. Hub Discovery, Datasets, Tidymodels Integration – finding LLM models Hub. Text Classification – need structured labels rather free-form text.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Text Classification and Zero-Shot Labeling","text":"Text classification assigns one labels piece text. Common applications include sentiment analysis, spam detection, intent recognition, topic categorization. huggingfaceR provides two complementary approaches: hf_classify() models trained specific label sets, hf_classify_zero_shot() assigning arbitrary labels without task-specific training.","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"classifying-a-single-text","dir":"Articles","previous_headings":"Sentiment Analysis with hf_classify()","what":"Classifying a Single Text","title":"Text Classification and Zero-Shot Labeling","text":"hf_classify() sends text pre-trained classification model returns tibble predicted label confidence score. default model (distilbert/distilbert-base-uncased-finetuned-sst-2-english) trained binary sentiment (POSITIVE/NEGATIVE). score column represents model’s confidence predicted label.","code":"hf_classify(\"I love using R for data science!\") #> # A tibble: 1 x 3 #>   text                              label    score #>   <chr>                             <chr>    <dbl> #> 1 I love using R for data science!  POSITIVE 0.999"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"classifying-multiple-texts","dir":"Articles","previous_headings":"Sentiment Analysis with hf_classify()","what":"Classifying Multiple Texts","title":"Text Classification and Zero-Shot Labeling","text":"Pass character vector classify several texts one call. result tibble one row per input text.","code":"reviews <- c(  \"This product exceeded my expectations\",  \"Terrible customer service, never again\",  \"It works fine, nothing remarkable\",  \"Absolutely brilliant design\",  \"Waste of money\" )  hf_classify(reviews)"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"using-alternative-models","dir":"Articles","previous_headings":"Sentiment Analysis with hf_classify()","what":"Using Alternative Models","title":"Text Classification and Zero-Shot Labeling","text":"text-classification model Hub can used specifying model parameter. Use hf_search_models() discover options.","code":"# Find emotion detection models hf_search_models(task = \"text-classification\", search = \"emotion\", limit = 5)  # Use a multi-class emotion model hf_classify(   \"I can't believe we won the championship!\",   model = \"j-hartmann/emotion-english-distilroberta-base\" )"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"zero-shot-classification-with-hf_classify_zero_shot","dir":"Articles","previous_headings":"","what":"Zero-Shot Classification with hf_classify_zero_shot()","title":"Text Classification and Zero-Shot Labeling","text":"Zero-shot classification lets define label set inference time. model determines labels best describe input text without requiring task-specific training data.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"custom-categories","dir":"Articles","previous_headings":"Zero-Shot Classification with hf_classify_zero_shot()","what":"Custom Categories","title":"Text Classification and Zero-Shot Labeling","text":"result contains one row per label, sorted confidence. model (facebook/bart-large-mnli default) evaluates well label describes input.","code":"hf_classify_zero_shot(   \"The Federal Reserve raised interest rates by 25 basis points\",   labels = c(\"economics\", \"politics\", \"technology\", \"sports\") ) #> # A tibble: 4 x 3 #>   text                                                      label      score #>   <chr>                                                     <chr>      <dbl> #> 1 The Federal Reserve raised interest rates by 25 basis ... economics  0.85 #> 2 The Federal Reserve raised interest rates by 25 basis ... politics   0.10 #> 3 The Federal Reserve raised interest rates by 25 basis ... technology 0.03 #> 4 The Federal Reserve raised interest rates by 25 basis ... sports     0.02"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"multi-label-classification","dir":"Articles","previous_headings":"Zero-Shot Classification with hf_classify_zero_shot()","what":"Multi-Label Classification","title":"Text Classification and Zero-Shot Labeling","text":"text might belong multiple categories simultaneously, set multi_label = TRUE. multi-label mode, scores independent – need sum 1.","code":"hf_classify_zero_shot(   \"This laptop has amazing graphics and runs all my games smoothly\",   labels = c(\"technology\", \"gaming\", \"business\", \"entertainment\"),   multi_label = TRUE )"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"classifying-multiple-texts-1","dir":"Articles","previous_headings":"Zero-Shot Classification with hf_classify_zero_shot()","what":"Classifying Multiple Texts","title":"Text Classification and Zero-Shot Labeling","text":"hf_classify_zero_shot() accepts character vector. text classified label set.","code":"headlines <- c(   \"Stock markets reach all-time highs\",   \"New vaccine shows 95% efficacy in trials\",   \"Championship finals draw record viewership\" )  hf_classify_zero_shot(   headlines,   labels = c(\"finance\", \"health\", \"sports\", \"politics\") )"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"tips-for-choosing-labels","dir":"Articles","previous_headings":"Zero-Shot Classification with hf_classify_zero_shot()","what":"Tips for Choosing Labels","title":"Text Classification and Zero-Shot Labeling","text":"quality zero-shot results depends heavily label wording: specific. “machine learning” works better “technology” ML-related texts. Use noun phrases. “customer complaint” outperforms “bad” “negative.” Match text register. academic texts, use formal labels; social media, use colloquial ones. Experiment. Try synonyms rephrasings – small changes can noticeably affect scores.","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"adding-sentiment-to-a-data-frame","dir":"Articles","previous_headings":"Data Frame Workflows","what":"Adding Sentiment to a Data Frame","title":"Text Classification and Zero-Shot Labeling","text":"common pattern classify text column add results back original data.","code":"customer_reviews <- tibble(   review_id = 1:6,   product = c(\"Widget A\", \"Widget A\", \"Widget B\",               \"Widget B\", \"Widget C\", \"Widget C\"),   text = c(     \"Works perfectly, great build quality\",     \"Stopped working after a month\",     \"Good value for the price\",     \"Flimsy materials, disappointed\",     \"Best purchase I've made this year\",     \"Does the job but nothing special\"   ) )  # Classify and join back customer_reviews |>   mutate(sentiment = hf_classify(text)) |>   unnest(sentiment, names_sep = \"_\") |>   select(review_id, product, text, sentiment_label, sentiment_score)"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"categorizing-support-tickets","dir":"Articles","previous_headings":"Data Frame Workflows","what":"Categorizing Support Tickets","title":"Text Classification and Zero-Shot Labeling","text":"Zero-shot classification well-suited routing tagging workflows categories may change time.","code":"tickets <- tibble(   ticket_id = 101:106,   message = c(     \"I can't log into my account\",     \"Please cancel my subscription\",     \"The app crashes when I open settings\",     \"How do I update my payment method?\",     \"Your product is great, just wanted to say thanks\",     \"I was charged twice for my order\"   ) )  # Classify all messages against the label set category_results <- hf_classify_zero_shot(   tickets$message,   labels = c(\"account access\", \"billing\", \"bug report\",              \"cancellation\", \"feedback\") )  # Keep the top category for each ticket categorized <- category_results |>   group_by(text) |>   slice_max(score, n = 1) |>   ungroup() |>   left_join(tickets, by = c(\"text\" = \"message\")) |>   select(ticket_id, message = text, category = label, confidence = score)  categorized"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"summarizing-by-category","dir":"Articles","previous_headings":"Data Frame Workflows","what":"Summarizing by Category","title":"Text Classification and Zero-Shot Labeling","text":"texts classified, standard dplyr verbs work expected.","code":"categorized |>   count(category, sort = TRUE)  categorized |>   group_by(category) |>   summarise(     n = n(),     avg_confidence = mean(confidence)   )"},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"choosing-the-right-model","dir":"Articles","previous_headings":"","what":"Choosing the Right Model","title":"Text Classification and Zero-Shot Labeling","text":"default models provide strong general-purpose performance, specialized models often perform better domain-specific tasks. brief guide: Use hf_search_models(task = \"text-classification\") browse available models. See Hub Discovery vignette advanced search techniques.","code":""},{"path":"https://farach.github.io/huggingfaceR/articles/text-classification.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Text Classification and Zero-Shot Labeling","text":"Getting Started – installation authentication. Hub Discovery, Datasets, Tidymodels Integration – finding models building ML pipelines.","code":""},{"path":"https://farach.github.io/huggingfaceR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Alex Farach. Maintainer. Sam Terfa. Author. Jack Penzer. Author.","code":""},{"path":"https://farach.github.io/huggingfaceR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Terfa S, Penzer J (2026). huggingfaceR: Hugging Face State---Art Models R. R package version 2.0.0.","code":"@Manual{,   title = {huggingfaceR: Hugging Face State-of-the-Art Models in R},   author = {Sam Terfa and Jack Penzer},   year = {2026},   note = {R package version 2.0.0}, }"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"huggingfacer-","dir":"","previous_headings":"","what":"huggingfaceR - API-first ML for R","title":"huggingfaceR - API-first ML for R","text":"API-first R package accessing 500,000+ machine learning models, embeddings, datasets Hugging Face Hub. Python required.","code":""},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"huggingfaceR - API-first ML for R","text":"","code":"# install.packages(\"devtools\") devtools::install_github(\"farach/huggingfaceR\")"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"setup","dir":"","previous_headings":"","what":"Setup","title":"huggingfaceR - API-first ML for R","text":"Get free API token huggingface.co/settings/tokens, configure R:","code":"library(huggingfaceR)  hf_set_token(\"hf_your_token_here\", store = TRUE) hf_whoami()"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"text-classification","dir":"","previous_headings":"","what":"Text Classification","title":"huggingfaceR - API-first ML for R","text":"","code":"# Sentiment analysis hf_classify(\"I love using R for data science!\") #> # A tibble: 1 x 3 #>   text                              label    score #>   <chr>                             <chr>    <dbl> #> 1 I love using R for data science!  POSITIVE 0.999  # Zero-shot classification with custom labels hf_classify_zero_shot(   \"I just bought a new laptop for coding\",   labels = c(\"technology\", \"sports\", \"politics\", \"food\") )"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"embeddings-and-similarity","dir":"","previous_headings":"","what":"Embeddings and Similarity","title":"huggingfaceR - API-first ML for R","text":"","code":"sentences <- c(   \"The cat sat on the mat\",   \"A feline rested on the rug\",   \"The dog played in the park\" )  embeddings <- hf_embed(sentences) embeddings #> # A tibble: 3 x 3 #>   text                        embedding    n_dims #>   <chr>                       <list>        <int> #> 1 The cat sat on the mat      <dbl [384]>     384 #> 2 A feline rested on the rug  <dbl [384]>     384 #> 3 The dog played in the park  <dbl [384]>     384  hf_similarity(embeddings) #> # A tibble: 3 x 3 #>   text_1                  text_2                      similarity #>   <chr>                   <chr>                            <dbl> #> 1 The cat sat on the mat  A feline rested on the rug       0.89 #> 2 The cat sat on the mat  The dog played in the park       0.45 #> 3 A feline rested on ...  The dog played in the park       0.39"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"chat-with-open-source-llms","dir":"","previous_headings":"","what":"Chat with Open-Source LLMs","title":"huggingfaceR - API-first ML for R","text":"","code":"hf_chat(\"What is the tidyverse?\")  # With a system prompt hf_chat(   \"Explain logistic regression in two sentences.\",   system = \"You are a statistics instructor. Use plain language.\" )  # Multi-turn conversation convo <- hf_conversation(system = \"You are a helpful R tutor.\") convo <- chat(convo, \"How do I read a CSV file?\") convo <- chat(convo, \"What about Excel files?\")"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"text-generation","dir":"","previous_headings":"","what":"Text Generation","title":"huggingfaceR - API-first ML for R","text":"","code":"hf_generate(\"Once upon a time in a land far away,\", max_new_tokens = 100)  hf_fill_mask(\"The capital of France is [MASK].\") #> # A tibble: 5 x 4 #>   text                              token   score filled #>   <chr>                             <chr>   <dbl> <chr> #> 1 The capital of France is [MASK].  paris   0.88  The capital of France is paris. #> 2 The capital of France is [MASK].  lyon    0.03  The capital of France is lyon. #> ..."},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"tidyverse-integration","dir":"","previous_headings":"","what":"Tidyverse Integration","title":"huggingfaceR - API-first ML for R","text":"functions accept character vectors return tibbles.","code":"library(dplyr) library(tidyr)  reviews <- tibble(   id = 1:3,   text = c(     \"This product is amazing!\",     \"Terrible experience.\",     \"It's okay, nothing special.\"   ) )  reviews |>   mutate(sentiment = hf_classify(text)) |>   unnest(sentiment, names_sep = \"_\") |>   select(id, text, sentiment_label, sentiment_score)"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"tidymodels","dir":"","previous_headings":"Tidyverse Integration","what":"Tidymodels","title":"huggingfaceR - API-first ML for R","text":"Use embeddings features machine learning workflows:","code":"library(tidymodels)  rec <- recipe(sentiment ~ text, data = train_data) |>   step_hf_embed(text)  wf <- workflow() |>   add_recipe(rec) |>   add_model(logistic_reg()) |>   fit(data = train_data)"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"tidytext","dir":"","previous_headings":"Tidyverse Integration","what":"Tidytext","title":"huggingfaceR - API-first ML for R","text":"Semantic search document clustering:","code":"docs |>   hf_embed_text(text) |>   hf_nearest_neighbors(\"machine learning\", k = 5)  docs |>   hf_embed_text(text) |>   hf_cluster_texts(k = 3) |>   hf_extract_topics(text_col = \"text\", k = 3)"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"hub-and-datasets","dir":"","previous_headings":"","what":"Hub and Datasets","title":"huggingfaceR - API-first ML for R","text":"","code":"# Search models hf_search_models(task = \"text-classification\", limit = 10)  # Load datasets into tibbles (no Python needed) imdb <- hf_load_dataset(\"imdb\", split = \"train\", limit = 1000)"},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn More","title":"huggingfaceR - API-first ML for R","text":"vignette(\"getting-started\") – setup first examples vignette(\"text-classification\") – sentiment analysis zero-shot labeling vignette(\"embeddings--similarity\") – semantic search, clustering, visualization vignette(\"llm-chat--generation\") – conversations text generation vignette(\"hub-datasets--modeling\") – Hub discovery tidymodels pipelines vignette(\"anthropic-economic-index\") – AI productivity research Anthropic Economic Index","code":""},{"path":"https://farach.github.io/huggingfaceR/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"huggingfaceR - API-first ML for R","text":"MIT","code":""},{"path":"https://farach.github.io/huggingfaceR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 huggingfaceR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Continue a Conversation — chat","title":"Continue a Conversation — chat","text":"Add message existing conversation get response.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Continue a Conversation — chat","text":"","code":"chat(conversation, message, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Continue a Conversation — chat","text":"conversation hf_conversation object hf_conversation(). message Character string. user message. ... Additional parameters passed model.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Continue a Conversation — chat","text":"Updated conversation object new messages history.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Continue a Conversation — chat","text":"","code":"if (FALSE) { # \\dontrun{ convo <- hf_conversation() convo <- chat(convo, \"Tell me a joke\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/grapes-or-or-grapes.html","id":null,"dir":"Reference","previous_headings":"","what":"Null-coalescing operator — %||%","title":"Null-coalescing operator — %||%","text":"Null-coalescing operator","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/grapes-or-or-grapes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Null-coalescing operator — %||%","text":"","code":"x %||% y"},{"path":"https://farach.github.io/huggingfaceR/reference/grapes-or-or-grapes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Null-coalescing operator — %||%","text":"x First value y Second value (default)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_api_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a Request to Hugging Face Inference API — hf_api_request","title":"Make a Request to Hugging Face Inference API — hf_api_request","text":"Internal helper function construct execute API requests.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_api_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a Request to Hugging Face Inference API — hf_api_request","text":"","code":"hf_api_request(   model_id,   inputs,   parameters = NULL,   token = NULL,   wait_for_model = TRUE,   use_cache = TRUE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_api_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a Request to Hugging Face Inference API — hf_api_request","text":"model_id Character string. model ID Hugging Face Hub. inputs input data (usually character vector list). parameters Optional list parameters inference. token Character string NULL. API token authentication. wait_for_model Logical. Wait model load ready. use_cache Logical. Use cached results identical inputs.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_api_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a Request to Hugging Face Inference API — hf_api_request","text":"raw response API.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"LLM Chat Interface — hf_chat","title":"LLM Chat Interface — hf_chat","text":"conversation open-source language model via Inference Providers API.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LLM Chat Interface — hf_chat","text":"","code":"hf_chat(   message,   system = NULL,   model = \"HuggingFaceTB/SmolLM3-3B\",   max_tokens = 500,   temperature = 0.7,   token = NULL,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LLM Chat Interface — hf_chat","text":"message Character string. user message send model. system Character string NULL. Optional system prompt set behavior. model Character string. Model ID Hugging Face Hub. Default: \"HuggingFaceTB/SmolLM3-3B\". Use `:provider` suffix select specific provider (e.g., \"meta-llama/Llama-3-8B-Instruct:together\"). max_tokens Integer. Maximum tokens generate. Default: 500. temperature Numeric. Sampling temperature (0-2). Default: 0.7. token Character string NULL. API token authentication. ... Additional parameters passed model.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"LLM Chat Interface — hf_chat","text":"tibble columns: role, content, model, tokens_used","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"LLM Chat Interface — hf_chat","text":"","code":"if (FALSE) { # \\dontrun{ # Simple question hf_chat(\"What is the capital of France?\")  # With system prompt hf_chat(   \"Explain gradient descent\",   system = \"You are a statistics professor. Use simple analogies.\" )  # Use a specific provider hf_chat(\"Hello!\", model = \"meta-llama/Llama-3-8B-Instruct:together\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Classification — hf_classify","title":"Text Classification — hf_classify","text":"Classify text using Hugging Face model. Commonly used sentiment analysis, topic classification, etc.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Classification — hf_classify","text":"","code":"hf_classify(   text,   model = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",   token = NULL,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Classification — hf_classify","text":"text Character vector text(s) classify. model Character string. Model ID Hugging Face Hub. Default: \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\" (sentiment analysis). token Character string NULL. API token authentication. ... Additional arguments (currently unused).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text Classification — hf_classify","text":"tibble columns: text, label, score","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Text Classification — hf_classify","text":"","code":"if (FALSE) { # \\dontrun{ # Sentiment analysis hf_classify(\"I love R programming!\")  # Multiple texts hf_classify(c(\"This is great!\", \"This is terrible.\"))  # Use in a pipeline library(dplyr) reviews |>   mutate(sentiment = hf_classify(review_text)) |>   unnest(sentiment) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify_zero_shot.html","id":null,"dir":"Reference","previous_headings":"","what":"Zero-Shot Classification — hf_classify_zero_shot","title":"Zero-Shot Classification — hf_classify_zero_shot","text":"Classify text custom categories without training model. model determines labels best describe input text.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify_zero_shot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Zero-Shot Classification — hf_classify_zero_shot","text":"","code":"hf_classify_zero_shot(   text,   labels,   model = \"facebook/bart-large-mnli\",   multi_label = FALSE,   token = NULL,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify_zero_shot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Zero-Shot Classification — hf_classify_zero_shot","text":"text Character vector text(s) classify. labels Character vector candidate labels/categories. model Character string. Model ID Hugging Face Hub. Default: \"facebook/bart-large-mnli\" multi_label Logical. TRUE, allows multiple labels per text. Default: FALSE (single label per text). token Character string NULL. API token authentication. ... Additional arguments (currently unused).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify_zero_shot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Zero-Shot Classification — hf_classify_zero_shot","text":"tibble columns: text, label, score (sorted score descending)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_classify_zero_shot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Zero-Shot Classification — hf_classify_zero_shot","text":"","code":"if (FALSE) { # \\dontrun{ # Classify into custom categories hf_classify_zero_shot(   \"I just bought a new laptop\",   labels = c(\"technology\", \"sports\", \"politics\", \"food\") )  # Multi-label classification hf_classify_zero_shot(   \"This laptop is great for gaming\",   labels = c(\"technology\", \"gaming\", \"entertainment\"),   multi_label = TRUE ) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_cluster_texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Cluster Texts by Semantic Similarity — hf_cluster_texts","title":"Cluster Texts by Semantic Similarity — hf_cluster_texts","text":"Perform k-means clustering text embeddings.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_cluster_texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cluster Texts by Semantic Similarity — hf_cluster_texts","text":"","code":"hf_cluster_texts(data, k = 3, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_cluster_texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cluster Texts by Semantic Similarity — hf_cluster_texts","text":"data data frame 'embedding' column (hf_embed_text). k Integer. Number clusters. Default: 3. ... Additional arguments passed stats::kmeans().","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_cluster_texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cluster Texts by Semantic Similarity — hf_cluster_texts","text":"input data frame added 'cluster' column.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_cluster_texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cluster Texts by Semantic Similarity — hf_cluster_texts","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2)  # Cluster documents docs_clustered <- docs_embedded |>   hf_cluster_texts(k = 3)  # Reduce dimensions and visualize library(uwot) emb_matrix <- do.call(rbind, docs_clustered$embedding) coords <- umap(emb_matrix)  docs_clustered |>   mutate(umap_1 = coords[, 1], umap_2 = coords[, 2]) |>   ggplot(aes(umap_1, umap_2, color = factor(cluster))) +   geom_point(size = 3) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_conversation.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-turn Conversation — hf_conversation","title":"Multi-turn Conversation — hf_conversation","text":"Create manage multi-turn conversation LLM.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_conversation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-turn Conversation — hf_conversation","text":"","code":"hf_conversation(system = NULL, model = \"HuggingFaceTB/SmolLM3-3B\")"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_conversation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-turn Conversation — hf_conversation","text":"system Character string NULL. System prompt conversation. model Character string. Model ID Hugging Face Hub. Default: \"HuggingFaceTB/SmolLM3-3B\".","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_conversation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-turn Conversation — hf_conversation","text":"conversation object (list) can extended chat().","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_conversation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-turn Conversation — hf_conversation","text":"","code":"if (FALSE) { # \\dontrun{ # Create conversation convo <- hf_conversation(system = \"You are a helpful R tutor.\")  # Add messages (see chat() method) convo <- chat(convo, \"How do I read a CSV?\") convo <- chat(convo, \"What about Excel files?\")  # View history convo$history } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_dataset_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Dataset Information — hf_dataset_info","title":"Get Dataset Information — hf_dataset_info","text":"Retrieve metadata dataset Hugging Face Hub.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_dataset_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Dataset Information — hf_dataset_info","text":"","code":"hf_dataset_info(dataset, token = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_dataset_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Dataset Information — hf_dataset_info","text":"dataset Character string. Dataset name. token Character string NULL. API token private datasets.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_dataset_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Dataset Information — hf_dataset_info","text":"list dataset information.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_dataset_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Dataset Information — hf_dataset_info","text":"","code":"if (FALSE) { # \\dontrun{ hf_dataset_info(\"imdb\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_detect_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect Default Config for a Dataset — hf_detect_config","title":"Detect Default Config for a Dataset — hf_detect_config","text":"Query splits endpoint find default config given dataset split.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_detect_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect Default Config for a Dataset — hf_detect_config","text":"","code":"hf_detect_config(dataset, split, token = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_detect_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect Default Config for a Dataset — hf_detect_config","text":"dataset Character string. Dataset name. split Character string. split find config . token Character string NULL. API token.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_detect_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect Default Config for a Dataset — hf_detect_config","text":"Character string config name.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Text Embeddings — hf_embed","title":"Generate Text Embeddings — hf_embed","text":"Generate dense vector representations (embeddings) text using transformer models. Useful semantic similarity, clustering, features ML models.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Text Embeddings — hf_embed","text":"","code":"hf_embed(text, model = \"BAAI/bge-small-en-v1.5\", token = NULL, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Text Embeddings — hf_embed","text":"text Character vector text(s) embed. model Character string. Model ID Hugging Face Hub. Default: \"BAAI/bge-small-en-v1.5\" (384-dim embeddings). token Character string NULL. API token authentication. ... Additional arguments (currently unused).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Text Embeddings — hf_embed","text":"tibble columns: text, embedding (list-column numeric vectors), n_dims","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Text Embeddings — hf_embed","text":"","code":"if (FALSE) { # \\dontrun{ # Generate embeddings embeddings <- hf_embed(c(\"Hello world\", \"Goodbye world\"))  # Access embedding vectors embeddings$embedding[[1]]  # First embedding vector } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Embed Text with Tidytext Compatibility — hf_embed_text","title":"Embed Text with Tidytext Compatibility — hf_embed_text","text":"Generate embeddings text tidy data frame. Designed work seamlessly tidytext workflows.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embed Text with Tidytext Compatibility — hf_embed_text","text":"","code":"hf_embed_text(   data,   text_col,   model = \"BAAI/bge-small-en-v1.5\",   token = NULL,   keep_text = TRUE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embed Text with Tidytext Compatibility — hf_embed_text","text":"data data frame tibble. text_col Unquoted column name containing text embed. model Character string. Hugging Face model ID embeddings. Default: \"BAAI/bge-small-en-v1.5\". token Character string NULL. API token authentication. keep_text Logical. Keep original text column? Default: TRUE.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embed Text with Tidytext Compatibility — hf_embed_text","text":"input data frame added embedding n_dims columns.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Embed Text with Tidytext Compatibility — hf_embed_text","text":"","code":"if (FALSE) { # \\dontrun{ library(dplyr) library(tidytext)  # Embed documents docs <- tibble(   doc_id = 1:3,   text = c(\"I love R\", \"Python is great\", \"Julia is fast\") )  docs_embedded <- docs |>   hf_embed_text(text)  # Find similar documents docs_embedded |>   hf_nearest_neighbors(\"I love R\", k = 2) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_umap.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction with UMAP — hf_embed_umap","title":"Dimensionality Reduction with UMAP — hf_embed_umap","text":"Reduce embedding dimensions 2D using UMAP visualization. Requires 'uwot' package installed.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_umap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction with UMAP — hf_embed_umap","text":"","code":"hf_embed_umap(   text,   model = \"BAAI/bge-small-en-v1.5\",   token = NULL,   n_neighbors = 15,   min_dist = 0.1,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_umap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction with UMAP — hf_embed_umap","text":"text Character vector text(s) embed reduce. model Character string. Model ID generating embeddings. Default: \"BAAI/bge-small-en-v1.5\". token Character string NULL. API token authentication. n_neighbors Integer. UMAP n_neighbors parameter. Default: 15. min_dist Numeric. UMAP min_dist parameter. Default: 0.1. ... Additional arguments passed uwot::umap().","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_umap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction with UMAP — hf_embed_umap","text":"tibble columns: text, umap_1, umap_2","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_embed_umap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction with UMAP — hf_embed_umap","text":"","code":"if (FALSE) { # \\dontrun{ # Reduce and visualize library(ggplot2) texts <- c(\"cat\", \"dog\", \"kitten\", \"puppy\", \"car\", \"truck\") coords <- hf_embed_umap(texts)  ggplot(coords, aes(umap_1, umap_2, label = text)) +   geom_text() +   theme_minimal() } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_extract_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Semantic Topics from Text — hf_extract_topics","title":"Extract Semantic Topics from Text — hf_extract_topics","text":"Identify semantic topics clustering embeddings extracting representative keywords cluster.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_extract_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Semantic Topics from Text — hf_extract_topics","text":"","code":"hf_extract_topics(data, text_col = \"text\", k = 5, top_n = 10)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_extract_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Semantic Topics from Text — hf_extract_topics","text":"data data frame text embeddings. text_col Character string. Name text column. k Integer. Number topics/clusters. Default: 5. top_n Integer. Number top words per topic. Default: 10.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_extract_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Semantic Topics from Text — hf_extract_topics","text":"tibble topics top terms.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_extract_topics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Semantic Topics from Text — hf_extract_topics","text":"","code":"if (FALSE) { # \\dontrun{ library(tidytext)  # Extract topics topics <- docs_embedded |>   hf_extract_topics(text_col = \"text\", k = 3, top_n = 5) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Conversational Agent — hf_ez_conversational","title":"Create a Conversational Agent — hf_ez_conversational","text":"task corresponds chatbot like structure. Models tend shorter max_length, please check caution using given model need long range dependency .","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Conversational Agent — hf_ez_conversational","text":"","code":"hf_ez_conversational(model_id = \"microsoft/DialoGPT-large\", use_api = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Conversational Agent — hf_ez_conversational","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'microsoft/DialoGPT-large'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Conversational Agent — hf_ez_conversational","text":"conversational object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Conversational Agent — hf_ez_conversational","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default model ez <- hf_ez_conversational()  # Continue the conversation ez$infer(past_user_inputs = list(\"Which movie is the best ?\"), generated_responses = list(\"It's Die Hard for sure.\"), text = \"Can you explain why?\", min_length = 10, max_length = 50) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Conversational API Inference — hf_ez_conversational_api_inference","title":"Conversational API Inference — hf_ez_conversational_api_inference","text":"Conversational API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conversational API Inference — hf_ez_conversational_api_inference","text":"","code":"hf_ez_conversational_api_inference(   text,   generated_responses = NULL,   past_user_inputs = NULL,   min_length = NULL,   max_length = NULL,   top_k = NULL,   top_p = NULL,   temperature = 1,   max_time = NULL,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conversational API Inference — hf_ez_conversational_api_inference","text":"text last input user conversation. generated_responses list strings corresponding earlier replies model. past_user_inputs list strings corresponding earlier replies user. length generated_responses. min_length (Default: None). Integer define minimum length tokens output summary. max_length (Default: None). Integer define maximum length tokens output summary. top_k (Default: None). Integer define top tokens considered within sample operation create new text. top_p (Default: None). Float define tokens within sample operation text generation. Add tokens sample probable least probable sum probabilities greater top_p. temperature (Default: 1.0). Float (0.0-100.0). temperature sampling operation. 1 means regular sampling, 0 means always take highest score, 100.0 getting closer uniform probability. max_time (Default: None). Float (0-120.0). amount time seconds query take maximum. Network can cause overhead soft limit. tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error). repetition_penalty (Default: None). Float (0.0-100.0). token used within generation penalized picked successive generation passes.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conversational API Inference — hf_ez_conversational_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Conversational Local Inference — hf_ez_conversational_local_inference","title":"Conversational Local Inference — hf_ez_conversational_local_inference","text":"Conversational Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conversational Local Inference — hf_ez_conversational_local_inference","text":"","code":"hf_ez_conversational_local_inference(   text,   generated_responses = NULL,   past_user_inputs = NULL,   min_length = NULL,   max_length = NULL,   top_k = NULL,   top_p = NULL,   temperature = 1,   max_time = NULL,   tidy = TRUE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conversational Local Inference — hf_ez_conversational_local_inference","text":"text last input user conversation. generated_responses list strings corresponding earlier replies model. past_user_inputs list strings corresponding earlier replies user. length generated_responses. min_length (Default: None). Integer define minimum length tokens output summary. max_length (Default: None). Integer define maximum length tokens output summary. top_k (Default: None). Integer define top tokens considered within sample operation create new text. top_p (Default: None). Float define tokens within sample operation text generation. Add tokens sample probable least probable sum probabilities greater top_p. temperature (Default: 1.0). Float (0.0-100.0). temperature sampling operation. 1 means regular sampling, 0 means always take highest score, 100.0 getting closer uniform probability. max_time (Default: None). Float (0-120.0). amount time seconds query take maximum. Network can cause overhead soft limit. repetition_penalty (Default: None). Float (0.0-100.0). token used within generation penalized picked successive generation passes.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_conversational_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conversational Local Inference — hf_ez_conversational_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Fill-in-the-Blank Tasks — hf_ez_fill_mask","title":"Perform Fill-in-the-Blank Tasks — hf_ez_fill_mask","text":"Tries fill hole missing word (token precise). ’s base task BERT models.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Fill-in-the-Blank Tasks — hf_ez_fill_mask","text":"","code":"hf_ez_fill_mask(model_id = \"google-bert/bert-base-uncased\", use_api = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Fill-in-the-Blank Tasks — hf_ez_fill_mask","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'bert-base-uncased'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform Fill-in-the-Blank Tasks — hf_ez_fill_mask","text":"fill mask object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform Fill-in-the-Blank Tasks — hf_ez_fill_mask","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default model and use local inference ez <- hf_ez_fill_mask() ez$infer(string = \"The answer to the universe is [MASK].\")  # Load a specific model and use the api for inference. Note the mask is different for different models. ez <- hf_ez_fill_mask(model_id = 'xlm-roberta-base', use_api = TRUE) ez$infer(string = \"The answer to the universe is <MASK>.\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill Mask API Inference — hf_ez_fill_mask_api_inference","title":"Fill Mask API Inference — hf_ez_fill_mask_api_inference","text":"Fill Mask API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill Mask API Inference — hf_ez_fill_mask_api_inference","text":"","code":"hf_ez_fill_mask_api_inference(   string,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill Mask API Inference — hf_ez_fill_mask_api_inference","text":"string string filled , must contain [MASK] token (check model card exact name mask) tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill Mask API Inference — hf_ez_fill_mask_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill Mask Local Inference — hf_ez_fill_mask_local_inference","title":"Fill Mask Local Inference — hf_ez_fill_mask_local_inference","text":"Fill Mask Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill Mask Local Inference — hf_ez_fill_mask_local_inference","text":"","code":"hf_ez_fill_mask_local_inference(string, tidy = TRUE, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill Mask Local Inference — hf_ez_fill_mask_local_inference","text":"string string filled , must contain [MASK] token (check model card exact name mask) tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_fill_mask_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill Mask Local Inference — hf_ez_fill_mask_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering.html","id":null,"dir":"Reference","previous_headings":"","what":"Answer Questions about a Text based on Context — hf_ez_question_answering","title":"Answer Questions about a Text based on Context — hf_ez_question_answering","text":"Want nice know--bot can answer question?","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Answer Questions about a Text based on Context — hf_ez_question_answering","text":"","code":"hf_ez_question_answering(   model_id = \"deepset/roberta-base-squad2\",   use_api = FALSE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Answer Questions about a Text based on Context — hf_ez_question_answering","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'deepset/roberta-base-squad2'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Answer Questions about a Text based on Context — hf_ez_question_answering","text":"question answering object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Answer Questions about a Text based on Context — hf_ez_question_answering","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default model and use local inference ez <- hf_ez_question_answering() ez$infer(question = \"What's my name?\", context = \"My name is Clara and I live in Berkeley.\")  # Use the api for inference. ez <- hf_ez_fill_mask(use_api = TRUE) ez$infer(question = \"What's my name?\", context = \"My name is Clara and I live in Berkeley.\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Question Answering API Inference — hf_ez_question_answering_api_inference","title":"Question Answering API Inference — hf_ez_question_answering_api_inference","text":"Question Answering API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Question Answering API Inference — hf_ez_question_answering_api_inference","text":"","code":"hf_ez_question_answering_api_inference(   question,   context,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question Answering API Inference — hf_ez_question_answering_api_inference","text":"question question answered based provided context context context consult answering question tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Question Answering API Inference — hf_ez_question_answering_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Question Answering Local Inference — hf_ez_question_answering_local_inference","title":"Question Answering Local Inference — hf_ez_question_answering_local_inference","text":"Question Answering Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Question Answering Local Inference — hf_ez_question_answering_local_inference","text":"","code":"hf_ez_question_answering_local_inference(question, context, tidy = TRUE, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question Answering Local Inference — hf_ez_question_answering_local_inference","text":"question question answered based provided context context context consult answering question tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_question_answering_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Question Answering Local Inference — hf_ez_question_answering_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare Sentence Similarity Semantically — hf_ez_sentence_similarity","title":"Compare Sentence Similarity Semantically — hf_ez_sentence_similarity","text":"Calculate semantic similarity one text list sentences comparing embeddings.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare Sentence Similarity Semantically — hf_ez_sentence_similarity","text":"","code":"hf_ez_sentence_similarity(   model_id = \"sentence-transformers/all-MiniLM-L6-v2\",   use_api = FALSE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare Sentence Similarity Semantically — hf_ez_sentence_similarity","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'sentence-transformers/-MiniLM-L6-v2'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare Sentence Similarity Semantically — hf_ez_sentence_similarity","text":"sentence similarity object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare Sentence Similarity Semantically — hf_ez_sentence_similarity","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default model and use local inference ez <- hf_ez_sentence_similarity() ez$infer(source_sentence = 'That is a happy person', sentences = list('That is a happy dog', 'That is a very happy person\", \"Today is a sunny day'))  # Use the api for inference. ez <- hf_ez_sentence_similarity(use_api = TRUE) ez$infer(source_sentence = 'That is a happy person', sentences = list('That is a happy dog', 'That is a very happy person', 'Today is a sunny day')) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Sentence Similarity API Inference — hf_ez_sentence_similarity_api_inference","title":"Sentence Similarity API Inference — hf_ez_sentence_similarity_api_inference","text":"Sentence Similarity API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sentence Similarity API Inference — hf_ez_sentence_similarity_api_inference","text":"","code":"hf_ez_sentence_similarity_api_inference(   source_sentence,   sentences,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sentence Similarity API Inference — hf_ez_sentence_similarity_api_inference","text":"source_sentence string wish compare strings . can phrase, sentence, longer passage, depending model used. sentences list strings compared source_sentence. tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sentence Similarity API Inference — hf_ez_sentence_similarity_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Sentence Similarity Local Inference — hf_ez_sentence_similarity_local_inference","title":"Sentence Similarity Local Inference — hf_ez_sentence_similarity_local_inference","text":"Sentence Similarity Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sentence Similarity Local Inference — hf_ez_sentence_similarity_local_inference","text":"","code":"hf_ez_sentence_similarity_local_inference(   source_sentence,   sentences,   tidy = TRUE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sentence Similarity Local Inference — hf_ez_sentence_similarity_local_inference","text":"source_sentence string wish compare strings . can phrase, sentence, longer passage, depending model used. sentences list strings compared source_sentence. tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_sentence_similarity_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sentence Similarity Local Inference — hf_ez_sentence_similarity_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize a Text — hf_ez_summarization","title":"Summarize a Text — hf_ez_summarization","text":"task well known summarize longer text shorter text. careful, models maximum length input. means summary handle full books instance. careful choosing model.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize a Text — hf_ez_summarization","text":"","code":"hf_ez_summarization(model_id = \"facebook/bart-large-cnn\", use_api = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize a Text — hf_ez_summarization","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'facebook/bart-large-cnn'. use_api Whether use Inference API run model (TRUE)  download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize a Text — hf_ez_summarization","text":"summarization object","code":""},{"path":[]},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarization API Inference — hf_ez_summarization_api_inference","title":"Summarization API Inference — hf_ez_summarization_api_inference","text":"Summarization API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarization API Inference — hf_ez_summarization_api_inference","text":"","code":"hf_ez_summarization_api_inference(   string,   min_length = NULL,   max_length = NULL,   top_k = NULL,   top_p = NULL,   temperature = 1,   repetition_penalty = NULL,   max_time = NULL,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarization API Inference — hf_ez_summarization_api_inference","text":"string string summarized min_length Integer define minimum length tokens output summary. Default: NULL max_length Integer define maximum length tokens output summary. Default: NULL top_k Integer define top tokens considered within sample operation create new text. Default: NULL top_p Float define tokens within sample operation text generation. Add tokens sample probable least probable sum probabilities greater top_p. Default: NULL temperature Float (0.0-100.0). temperature sampling operation. 1 means regular sampling, 0 means always take highest score, 100.0 getting closer uniform probability. Default: 1.0 repetition_penalty Float (0.0-100.0). token used within generation penalized picked successive generation passes. Default: NULL max_time Float (0-120.0). amount time seconds query take maximum. Network can cause overhead soft limit. Default: NULL tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarization API Inference — hf_ez_summarization_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarization Local Inference — hf_ez_summarization_local_inference","title":"Summarization Local Inference — hf_ez_summarization_local_inference","text":"Summarization Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarization Local Inference — hf_ez_summarization_local_inference","text":"","code":"hf_ez_summarization_local_inference(   string,   min_length = NULL,   max_length = NULL,   top_k = NULL,   top_p = NULL,   temperature = 1,   repetition_penalty = NULL,   max_time = NULL,   tidy = TRUE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarization Local Inference — hf_ez_summarization_local_inference","text":"string string summarized min_length Integer define minimum length tokens output summary. Default: NULL max_length Integer define maximum length tokens output summary. Default: NULL top_k Integer define top tokens considered within sample operation create new text. Default: NULL top_p Float define tokens within sample operation text generation. Add tokens sample probable least probable sum probabilities greater top_p. Default: NULL temperature Float (0.0-100.0). temperature sampling operation. 1 means regular sampling, 0 means always take highest score, 100.0 getting closer uniform probability. Default: 1.0 repetition_penalty Float (0.0-100.0). token used within generation penalized picked successive generation passes. Default: NULL max_time Float (0-120.0). amount time seconds query take maximum. Network can cause overhead soft limit. Default: NULL tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_summarization_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarization Local Inference — hf_ez_summarization_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering.html","id":null,"dir":"Reference","previous_headings":"","what":"Answer Questions about a Data Table — hf_ez_table_question_answering","title":"Answer Questions about a Data Table — hf_ez_table_question_answering","text":"Don’t know SQL? Don’t want dive large spreadsheet? Ask questions plain english!","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Answer Questions about a Data Table — hf_ez_table_question_answering","text":"","code":"hf_ez_table_question_answering(   model_id = \"google/tapas-base-finetuned-wtq\",   use_api = FALSE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Answer Questions about a Data Table — hf_ez_table_question_answering","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'google/tapas-base-finetuned-wtq'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Answer Questions about a Data Table — hf_ez_table_question_answering","text":"table question answering object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Answer Questions about a Data Table — hf_ez_table_question_answering","text":"","code":"if (FALSE) { # \\dontrun{ # Create a table to query qa_table <-   tibble::tibble(Repository = c('Transformers', 'Datasets', 'Tokenizers'),                  Stars = c('36542', '4512', '3934'),                  Contributors = c('651', '77', '34'),                  Programming.language = c('Python', 'Python', 'Rust, Python and NodeJS'))  # Load the default model and use local inference ez <- hf_ez_table_question_answering() ez$infer(query = \"How many stars does the transformers repository have?\", table = qa_table)  # Use the api for inference. ez <- hf_ez_fill_mask(use_api = TRUE) ez$infer(query = \"How many stars does the transformers repository have?\", table = qa_table) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Table Question Answering API Inference — hf_ez_table_question_answering_api_inference","title":"Table Question Answering API Inference — hf_ez_table_question_answering_api_inference","text":"Table Question Answering API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Table Question Answering API Inference — hf_ez_table_question_answering_api_inference","text":"","code":"hf_ez_table_question_answering_api_inference(   query,   table,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Table Question Answering API Inference — hf_ez_table_question_answering_api_inference","text":"query query plain text want ask table table dataframe text columns. tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Table Question Answering API Inference — hf_ez_table_question_answering_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Table Question Answering Local Inference — hf_ez_table_question_answering_local_inference","title":"Table Question Answering Local Inference — hf_ez_table_question_answering_local_inference","text":"Table Question Answering Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Table Question Answering Local Inference — hf_ez_table_question_answering_local_inference","text":"","code":"hf_ez_table_question_answering_local_inference(query, table, tidy = TRUE, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Table Question Answering Local Inference — hf_ez_table_question_answering_local_inference","text":"query query plain text want ask table table dataframe text columns. tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_table_question_answering_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Table Question Answering Local Inference — hf_ez_table_question_answering_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation.html","id":null,"dir":"Reference","previous_headings":"","what":"Answer General Questions — hf_ez_text2text_generation","title":"Answer General Questions — hf_ez_text2text_generation","text":"Essentially Text-generation task. uses Encoder-Decoder architecture, might change future options.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Answer General Questions — hf_ez_text2text_generation","text":"","code":"hf_ez_text2text_generation(model_id = \"google/flan-t5-large\", use_api = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Answer General Questions — hf_ez_text2text_generation","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'google/flan-t5-small'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Answer General Questions — hf_ez_text2text_generation","text":"text2text generation object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Answer General Questions — hf_ez_text2text_generation","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default model and use local inference ez <- hf_ez_text2text_generation() ez$infer(\"Please answer the following question. What is the boiling point of Nitrogen?\")  # Use the api for inference. ez <- hf_ez_text2text_generation(use_api = TRUE) ez$infer(\"Please answer the following question. What is the boiling point of Nitrogen?\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Text2Text Generation API Inference — hf_ez_text2text_generation_api_inference","title":"Text2Text Generation API Inference — hf_ez_text2text_generation_api_inference","text":"Text2Text Generation API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text2Text Generation API Inference — hf_ez_text2text_generation_api_inference","text":"","code":"hf_ez_text2text_generation_api_inference(   string,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text2Text Generation API Inference — hf_ez_text2text_generation_api_inference","text":"string general request model perform answer tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text2Text Generation API Inference — hf_ez_text2text_generation_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Text2Text Generation Local Inference — hf_ez_text2text_generation_local_inference","title":"Text2Text Generation Local Inference — hf_ez_text2text_generation_local_inference","text":"Text2Text Generation Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text2Text Generation Local Inference — hf_ez_text2text_generation_local_inference","text":"","code":"hf_ez_text2text_generation_local_inference(string, tidy = TRUE, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text2Text Generation Local Inference — hf_ez_text2text_generation_local_inference","text":"string general request model perform answer tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text2text_generation_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text2Text Generation Local Inference — hf_ez_text2text_generation_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify Texts into Pre-trained Categories — hf_ez_text_classification","title":"Classify Texts into Pre-trained Categories — hf_ez_text_classification","text":"Usually used sentiment-analysis output likelihood classes input.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify Texts into Pre-trained Categories — hf_ez_text_classification","text":"","code":"hf_ez_text_classification(   model_id = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",   use_api = FALSE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify Texts into Pre-trained Categories — hf_ez_text_classification","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'distilbert-base-uncased-finetuned-sst-2-english'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify Texts into Pre-trained Categories — hf_ez_text_classification","text":"text classification object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify Texts into Pre-trained Categories — hf_ez_text_classification","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default model and use local inference ez <- hf_ez_text_classification() ez$infer(string = c('I like you. I love you'), flatten = FALSE)  # Use the api for inference. ez <- hf_ez_text_classification(use_api = TRUE) ez$infer(string = c('I like you. I love you'), flatten = FALSE) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Classification API Inference — hf_ez_text_classification_api_inference","title":"Text Classification API Inference — hf_ez_text_classification_api_inference","text":"Text Classification API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Classification API Inference — hf_ez_text_classification_api_inference","text":"","code":"hf_ez_text_classification_api_inference(   string,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Classification API Inference — hf_ez_text_classification_api_inference","text":"string string classified tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text Classification API Inference — hf_ez_text_classification_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Classification Local Inference — hf_ez_text_classification_local_inference","title":"Text Classification Local Inference — hf_ez_text_classification_local_inference","text":"Text Classification Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Classification Local Inference — hf_ez_text_classification_local_inference","text":"","code":"hf_ez_text_classification_local_inference(string, tidy = TRUE, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Classification Local Inference — hf_ez_text_classification_local_inference","text":"string string classified tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_classification_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text Classification Local Inference — hf_ez_text_classification_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Text from a Prompt — hf_ez_text_generation","title":"Generate Text from a Prompt — hf_ez_text_generation","text":"Use continue text prompt. generic task.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Text from a Prompt — hf_ez_text_generation","text":"","code":"hf_ez_text_generation(model_id = \"openai-community/gpt2\", use_api = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Text from a Prompt — hf_ez_text_generation","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'gpt2'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Text from a Prompt — hf_ez_text_generation","text":"text generation object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Text from a Prompt — hf_ez_text_generation","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default model and use local inference ez <- hf_ez_text_generation() ez$infer(string = 'The answer to the universe is')  # Use the api for inference. ez <- hf_ez_text_generation(use_api = TRUE) ez$infer(string = 'The answer to the universe is') } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Generation API Inference — hf_ez_text_generation_api_inference","title":"Text Generation API Inference — hf_ez_text_generation_api_inference","text":"Text Generation API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Generation API Inference — hf_ez_text_generation_api_inference","text":"","code":"hf_ez_text_generation_api_inference(   string,   top_k = NULL,   top_p = NULL,   temperature = 1,   repetition_penalty = NULL,   max_new_tokens = NULL,   max_time = NULL,   return_full_text = TRUE,   num_return_sequences = 1L,   do_sample = TRUE,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Generation API Inference — hf_ez_text_generation_api_inference","text":"string string generated top_k (Default: None). Integer define top tokens considered within sample operation create new text. top_p (Default: None). Float define tokens within sample operation text generation. Add tokens sample probable least probable sum probabilities greater top_p temperature Float (0.0-100.0). temperature sampling operation. 1 means regular sampling, 0 means always take highest score, 100.0 getting closer uniform probability. Default: 1.0 repetition_penalty (Default: None). Float (0.0-100.0). token used within generation penalized picked successive generation passes. max_new_tokens (Default: None). Int (0-250). amount new tokens generated, include input length estimate size generated text want. new tokens slows request, look balance response times length text generated. max_time (Default: None). Float (0-120.0). amount time seconds query take maximum. Network can cause overhead soft limit. Use combination max_new_tokens best results. return_full_text (Default: True). Bool. set False, return results contain original query making easier prompting. num_return_sequences (Default: 1). Integer. number proposition want returned. do_sample (Optional: True). Bool. Whether use sampling, use greedy decoding otherwise.#' use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text Generation API Inference — hf_ez_text_generation_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Generation Local Inference — hf_ez_text_generation_local_inference","title":"Text Generation Local Inference — hf_ez_text_generation_local_inference","text":"Text Generation Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Generation Local Inference — hf_ez_text_generation_local_inference","text":"","code":"hf_ez_text_generation_local_inference(   string,   top_k = NULL,   top_p = NULL,   temperature = 1,   repetition_penalty = NULL,   max_new_tokens = NULL,   max_time = NULL,   return_full_text = TRUE,   num_return_sequences = 1L,   do_sample = TRUE,   tidy = TRUE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Generation Local Inference — hf_ez_text_generation_local_inference","text":"string string generated top_k (Default: None). Integer define top tokens considered within sample operation create new text. top_p (Default: None). Float define tokens within sample operation text generation. Add tokens sample probable least probable sum probabilities greater top_p temperature Float (0.0-100.0). temperature sampling operation. 1 means regular sampling, 0 means always take highest score, 100.0 getting closer uniform probability. Default: 1.0 repetition_penalty (Default: None). Float (0.0-100.0). token used within generation penalized picked successive generation passes. max_new_tokens (Default: None). Int (0-250). amount new tokens generated, include input length estimate size generated text want. new tokens slows request, look balance response times length text generated. max_time (Default: None). Float (0-120.0). amount time seconds query take maximum. Network can cause overhead soft limit. Use combination max_new_tokens best results. return_full_text (Default: True). Bool. set False, return results contain original query making easier prompting. num_return_sequences (Default: 1). Integer. number proposition want returned. do_sample (Optional: True). Bool. Whether use sampling, use greedy decoding otherwise.#'","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_text_generation_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text Generation Local Inference — hf_ez_text_generation_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify parts of a Text — hf_ez_token_classification","title":"Classify parts of a Text — hf_ez_token_classification","text":"Usually used sentence parsing, either grammatical, Named Entity Recognition (NER) understand keywords contained within text.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify parts of a Text — hf_ez_token_classification","text":"","code":"hf_ez_token_classification(   model_id = \"dbmdz/bert-large-cased-finetuned-conll03-english\",   use_api = FALSE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify parts of a Text — hf_ez_token_classification","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'dbmdz/bert-large-cased-finetuned-conll03-english'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify parts of a Text — hf_ez_token_classification","text":"text2text generation object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify parts of a Text — hf_ez_token_classification","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default named entity recognition model ez <- hf_ez_token_classification()  # Run NER. Note how the full name is aggregated into one named entity. ez$infer(string = \"My name is Sarah Jessica Parker but you can call me Jessica\", aggregation_strategy = 'simple')  # Run NER without aggregation. Note how the full name is separated into distinct named entities. ez$infer(string = \"My name is Sarah Jessica Parker but you can call me Jessica\", aggregation_strategy = 'none') } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Token Classification API Inference — hf_ez_token_classification_api_inference","title":"Token Classification API Inference — hf_ez_token_classification_api_inference","text":"Token Classification API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Token Classification API Inference — hf_ez_token_classification_api_inference","text":"","code":"hf_ez_token_classification_api_inference(   string,   aggregation_strategy = \"simple\",   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Token Classification API Inference — hf_ez_token_classification_api_inference","text":"string string classified aggregation_strategy (Default: simple). several aggregation strategies.  none: Every token gets classified without aggregation.   simple: Entities grouped according default schema (B-, - tags get merged tag similar).   first: simple strategy except words end different tags. Words use tag first token ambiguity.   average: simple strategy except words end different tags. Scores averaged across tokens maximum label applied.   max: simple strategy except words end different tags. Word entity token maximum score. tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Token Classification API Inference — hf_ez_token_classification_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Token Classification Local Inference — hf_ez_token_classification_local_inference","title":"Token Classification Local Inference — hf_ez_token_classification_local_inference","text":"Token Classification Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Token Classification Local Inference — hf_ez_token_classification_local_inference","text":"","code":"hf_ez_token_classification_local_inference(   string,   aggregation_strategy = \"simple\",   tidy = TRUE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Token Classification Local Inference — hf_ez_token_classification_local_inference","text":"string string classified aggregation_strategy (Default: simple). several aggregation strategies.  none: Every token gets classified without aggregation.   simple: Entities grouped according default schema (B-, - tags get merged tag similar).   first: simple strategy except words end different tags. Words use tag first token ambiguity.   average: simple strategy except words end different tags. Scores averaged across tokens maximum label applied.   max: simple strategy except words end different tags. Word entity token maximum score. tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_token_classification_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Token Classification Local Inference — hf_ez_token_classification_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation.html","id":null,"dir":"Reference","previous_headings":"","what":"Translate between Languages — hf_ez_translation","title":"Translate between Languages — hf_ez_translation","text":"task well known translate text one language another","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Translate between Languages — hf_ez_translation","text":"","code":"hf_ez_translation(model_id = \"Helsinki-NLP/opus-mt-en-es\", use_api = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Translate between Languages — hf_ez_translation","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'Helsinki-NLP/opus-mt-en-es'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Translate between Languages — hf_ez_translation","text":"translation object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Translate between Languages — hf_ez_translation","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default translation model ez <- hf_ez_translation()  # Translate from English to Spanish. ez$infer(string = \"My name is Sarah and I live in London\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Translation API Inference — hf_ez_translation_api_inference","title":"Translation API Inference — hf_ez_translation_api_inference","text":"Translation API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Translation API Inference — hf_ez_translation_api_inference","text":"","code":"hf_ez_translation_api_inference(   string,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Translation API Inference — hf_ez_translation_api_inference","text":"string string translated tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Translation API Inference — hf_ez_translation_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Translation Local Inference — hf_ez_translation_local_inference","title":"Translation Local Inference — hf_ez_translation_local_inference","text":"Translation Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Translation Local Inference — hf_ez_translation_local_inference","text":"","code":"hf_ez_translation_local_inference(string, tidy = TRUE, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Translation Local Inference — hf_ez_translation_local_inference","text":"string string translated tidy Whether tidy results tibble. Default: TRUE (tidy results)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_translation_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Translation Local Inference — hf_ez_translation_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Text Classification with No Context Required — hf_ez_zero_shot_classification","title":"Perform Text Classification with No Context Required — hf_ez_zero_shot_classification","text":"task super useful try classification zero code, simply pass sentence/paragraph possible labels sentence, get result.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Text Classification with No Context Required — hf_ez_zero_shot_classification","text":"","code":"hf_ez_zero_shot_classification(   model_id = \"facebook/bart-large-mnli\",   use_api = FALSE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Text Classification with No Context Required — hf_ez_zero_shot_classification","text":"model_id model_id. Run hf_search_models(...) model_ids. Defaults 'facebook/bart-large-mnli'. use_api Whether use Inference API run model (TRUE) download run model locally (FALSE). Defaults FALSE","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform Text Classification with No Context Required — hf_ez_zero_shot_classification","text":"zero shot classification object","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform Text Classification with No Context Required — hf_ez_zero_shot_classification","text":"","code":"if (FALSE) { # \\dontrun{ # Load the default model ez <- hf_ez_zero_shot_classification()  # Classify the string ez$infer(string = \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\", candidate_labels = c(\"refund\", \"legal\", \"faq\")) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification_api_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Zero Shot Classification API Inference — hf_ez_zero_shot_classification_api_inference","title":"Zero Shot Classification API Inference — hf_ez_zero_shot_classification_api_inference","text":"Zero Shot Classification API Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification_api_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Zero Shot Classification API Inference — hf_ez_zero_shot_classification_api_inference","text":"","code":"hf_ez_zero_shot_classification_api_inference(   string,   candidate_labels,   multi_label = FALSE,   tidy = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification_api_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Zero Shot Classification API Inference — hf_ez_zero_shot_classification_api_inference","text":"string string list strings candidate_labels list strings potential classes inputs. (max 10 candidate_labels, , simply run multiple requests, results going misleading using many candidate_labels anyway. want keep exact , can simply run multi_label=True scaling end. ) multi_label (Default: false) Boolean set True classes can overlap tidy Whether tidy results tibble. Default: TRUE (tidy results) use_gpu Whether use GPU inference. use_cache Whether use cached inference results previously seen inputs. wait_for_model Whether wait model ready instead receiving 503 error certain amount time. use_auth_token token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification_api_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Zero Shot Classification API Inference — hf_ez_zero_shot_classification_api_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification_local_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Zero Shot Classification Local Inference — hf_ez_zero_shot_classification_local_inference","title":"Zero Shot Classification Local Inference — hf_ez_zero_shot_classification_local_inference","text":"Zero Shot Classification Local Inference","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification_local_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Zero Shot Classification Local Inference — hf_ez_zero_shot_classification_local_inference","text":"","code":"hf_ez_zero_shot_classification_local_inference(   string,   candidate_labels,   multi_label = FALSE,   tidy = TRUE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification_local_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Zero Shot Classification Local Inference — hf_ez_zero_shot_classification_local_inference","text":"string string list strings candidate_labels list strings potential classes inputs. (max 10 candidate_labels, , simply run multiple requests, results going misleading using many candidate_labels anyway. want keep exact , can simply run multi_label=True scaling end. ) multi_label (Default: false) Boolean set True classes can overlap","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_ez_zero_shot_classification_local_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Zero Shot Classification Local Inference — hf_ez_zero_shot_classification_local_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill Mask — hf_fill_mask","title":"Fill Mask — hf_fill_mask","text":"Fill [MASK] token text predicted words. Commonly used BERT-style models.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill Mask — hf_fill_mask","text":"","code":"hf_fill_mask(   text,   model = \"google-bert/bert-base-uncased\",   mask_token = \"[MASK]\",   top_k = 5,   token = NULL,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill Mask — hf_fill_mask","text":"text Character vector text(s) containing [MASK] token. model Character string. Model ID Hugging Face Hub. Default: \"google-bert/bert-base-uncased\". mask_token Character string. mask token use. Default: \"[MASK]\". models use different tokens like \"<mask>\". top_k Integer. Number top predictions return. Default: 5. token Character string NULL. API token authentication. ... Additional arguments (currently unused).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill Mask — hf_fill_mask","text":"tibble columns: text, token, score, filled (complete text)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fill Mask — hf_fill_mask","text":"","code":"if (FALSE) { # \\dontrun{ # Fill in the blank hf_fill_mask(\"The capital of France is [MASK].\")  # Get top predictions hf_fill_mask(\"Paris is the [MASK] of France.\", top_k = 3)  # Use with different mask token hf_fill_mask(\"The capital of France is <mask>.\", mask_token = \"<mask>\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill Mask Task Payload — hf_fill_mask_payload","title":"Fill Mask Task Payload — hf_fill_mask_payload","text":"Tries fill hole missing word (token precise). ’s base task BERT models.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill Mask Task Payload — hf_fill_mask_payload","text":"","code":"hf_fill_mask_payload(string)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill Mask Task Payload — hf_fill_mask_payload","text":"string string filled , must contain [MASK] token (check model card exact name mask)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_fill_mask_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill Mask Task Payload — hf_fill_mask_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_generate.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Generation — hf_generate","title":"Text Generation — hf_generate","text":"Generate text prompt using language model via Inference Providers API.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_generate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Generation — hf_generate","text":"","code":"hf_generate(   prompt,   model = \"HuggingFaceTB/SmolLM3-3B\",   max_new_tokens = 50,   temperature = 1,   top_p = NULL,   token = NULL,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_generate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Generation — hf_generate","text":"prompt Character vector text prompt(s) generate . model Character string. Model ID Hugging Face Hub. Default: \"HuggingFaceTB/SmolLM3-3B\". max_new_tokens Integer. Maximum number tokens generate. Default: 50. temperature Numeric. Sampling temperature (0-2). Default: 1.0. top_p Numeric. Nucleus sampling parameter. Default: NULL. token Character string NULL. API token authentication. ... Additional parameters passed model.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_generate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text Generation — hf_generate","text":"tibble columns: prompt, generated_text","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_generate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Text Generation — hf_generate","text":"","code":"if (FALSE) { # \\dontrun{ # Simple text generation hf_generate(\"Once upon a time in a land far away,\")  # With different model hf_generate(\"The future of AI is\", model = \"meta-llama/Llama-3-8B-Instruct:together\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_get_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Hugging Face API Token — hf_get_token","title":"Get Hugging Face API Token — hf_get_token","text":"Internal function retrieve API token environment parameter.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_get_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Hugging Face API Token — hf_get_token","text":"","code":"hf_get_token(token = NULL, required = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_get_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Hugging Face API Token — hf_get_token","text":"token Character string NULL required Logical. TRUE, throws error token found.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_get_token.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Hugging Face API Token — hf_get_token","text":"Character string token, NULL found required.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_inference.html","id":null,"dir":"Reference","previous_headings":"","what":"Inference using a downloaded Hugging Face model or pipeline, or using the Inference API — hf_inference","title":"Inference using a downloaded Hugging Face model or pipeline, or using the Inference API — hf_inference","text":"model_id provided, Inference API used make prediction. wish download model pipeline rather running predictions Inference API, download model one hf_load_*_model() hf_load_pipeline() functions.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_inference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inference using a downloaded Hugging Face model or pipeline, or using the Inference API — hf_inference","text":"","code":"hf_inference(   model,   payload,   flatten = TRUE,   use_gpu = FALSE,   use_cache = FALSE,   wait_for_model = FALSE,   use_auth_token = NULL,   stop_on_error = FALSE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_inference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inference using a downloaded Hugging Face model or pipeline, or using the Inference API — hf_inference","text":"model Either downloaded model pipeline Hugging Face Hub (using hf_load_pipeline()), model_id. Run hf_search_models(...) model_ids. payload data predict . Use one hf_*_payload() functions create. flatten Whether flatten results data frame. Default: TRUE (flatten results) use_gpu API - Whether use GPU inference. use_cache API - Whether use cached inference results previously seen inputs. wait_for_model API - Whether wait model ready instead receiving 503 error certain amount time. use_auth_token API - token use HTTP bearer authorization Inference API. Defaults HUGGING_FACE_HUB_TOKEN environment variable. stop_on_error API - Whether throw error API error encountered. Defaults FALSE (throw error).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_inference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inference using a downloaded Hugging Face model or pipeline, or using the Inference API — hf_inference","text":"results inference","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_authors.html","id":null,"dir":"Reference","previous_headings":"","what":"List Authors — hf_list_authors","title":"List Authors — hf_list_authors","text":"List Model Authors","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_authors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Authors — hf_list_authors","text":"","code":"hf_list_authors(pattern = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_authors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Authors — hf_list_authors","text":"pattern search term regular expression. Defaults NULL (return results).","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_authors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Authors — hf_list_authors","text":"","code":"hf_list_authors(pattern = \"^sam\") #> character(0)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_datasets.html","id":null,"dir":"Reference","previous_headings":"","what":"List Datasets — hf_list_datasets","title":"List Datasets — hf_list_datasets","text":"List Model Datasets","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_datasets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Datasets — hf_list_datasets","text":"","code":"hf_list_datasets(pattern = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_datasets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Datasets — hf_list_datasets","text":"pattern search term regular expression. Defaults NULL (return results).","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_datasets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Datasets — hf_list_datasets","text":"","code":"hf_list_datasets(\"imdb\") #> Error in system2(uv, c(\"python list\", \"--all-versions\", \"--color never\",  :  #>   '\"C:\\Users\\alexfarach\\AppData\\Local\\R\\cache\\R\\reticulate\\uv\\bin\\uv.exe\"' not found #> character(0)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_languages.html","id":null,"dir":"Reference","previous_headings":"","what":"List Languages — hf_list_languages","title":"List Languages — hf_list_languages","text":"List Model Languages","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_languages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Languages — hf_list_languages","text":"","code":"hf_list_languages(pattern = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_languages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Languages — hf_list_languages","text":"pattern search term regular expression. Defaults NULL (return results).","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_languages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Languages — hf_list_languages","text":"","code":"hf_list_languages(\"es\") #> Error in system2(uv, c(\"python list\", \"--all-versions\", \"--color never\",  :  #>   '\"C:\\Users\\alexfarach\\AppData\\Local\\R\\cache\\R\\reticulate\\uv\\bin\\uv.exe\"' not found #> character(0)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_libraries.html","id":null,"dir":"Reference","previous_headings":"","what":"List Libraries — hf_list_libraries","title":"List Libraries — hf_list_libraries","text":"List Model Libraries","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_libraries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Libraries — hf_list_libraries","text":"","code":"hf_list_libraries(pattern = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_libraries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Libraries — hf_list_libraries","text":"pattern search term regular expression. Defaults NULL (return results).","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_libraries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Libraries — hf_list_libraries","text":"","code":"hf_list_libraries(\"pytorch|tensorflow\") #> Error in system2(uv, c(\"python list\", \"--all-versions\", \"--color never\",  :  #>   '\"C:\\Users\\alexfarach\\AppData\\Local\\R\\cache\\R\\reticulate\\uv\\bin\\uv.exe\"' not found #> character(0)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_licenses.html","id":null,"dir":"Reference","previous_headings":"","what":"List Licenses — hf_list_licenses","title":"List Licenses — hf_list_licenses","text":"List Model Licenses","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_licenses.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Licenses — hf_list_licenses","text":"","code":"hf_list_licenses(pattern = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_licenses.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Licenses — hf_list_licenses","text":"pattern search term regular expression. Defaults NULL (return results).","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_licenses.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Licenses — hf_list_licenses","text":"","code":"hf_list_licenses(\"mit\") #> Error in system2(uv, c(\"python list\", \"--all-versions\", \"--color never\",  :  #>   '\"C:\\Users\\alexfarach\\AppData\\Local\\R\\cache\\R\\reticulate\\uv\\bin\\uv.exe\"' not found #> character(0)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Models — hf_list_models","title":"List Models — hf_list_models","text":"List Model Names","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Models — hf_list_models","text":"","code":"hf_list_models(pattern = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Models — hf_list_models","text":"pattern search term regular expression. Defaults NULL (return results).","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Models — hf_list_models","text":"","code":"hf_list_models(\"bert-base-cased\") #> Error in system2(uv, c(\"python list\", \"--all-versions\", \"--color never\",  :  #>   '\"C:\\Users\\alexfarach\\AppData\\Local\\R\\cache\\R\\reticulate\\uv\\bin\\uv.exe\"' not found #> # A tibble: 0 × 1 #> # ℹ 1 variable: model <chr>"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_tasks.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Tasks — hf_list_tasks","title":"List Available Tasks — hf_list_tasks","text":"List available task types Hugging Face.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_tasks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Tasks — hf_list_tasks","text":"","code":"hf_list_tasks(pattern = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_tasks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Tasks — hf_list_tasks","text":"pattern Character string NULL. Optional regex pattern filter tasks.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_tasks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Tasks — hf_list_tasks","text":"character vector task names.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_list_tasks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Available Tasks — hf_list_tasks","text":"","code":"if (FALSE) { # \\dontrun{ # List all tasks hf_list_tasks()  # Filter tasks hf_list_tasks(pattern = \"classification\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_AutoModel_for_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Import a pre-trained AutoModel object for a specific task. — hf_load_AutoModel_for_task","title":"Import a pre-trained AutoModel object for a specific task. — hf_load_AutoModel_for_task","text":"Use function need load AutoModel specific task separate pipeline. model's config come ready-equipped specific task according input model_type.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_AutoModel_for_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import a pre-trained AutoModel object for a specific task. — hf_load_AutoModel_for_task","text":"","code":"hf_load_AutoModel_for_task(   model_type = \"AutoModelForSequenceClassification\",   model_id,   use_auth_token = NULL )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_AutoModel_for_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import a pre-trained AutoModel object for a specific task. — hf_load_AutoModel_for_task","text":"model_type AutoModel's type passed string e.g. c(\"AutoModelForQuestionAnswering\", \"AutoModelForTokenClassification\", \"AutoModelForSequenceClassification\") model_id model's name id Hugging Face hub use_auth_token private models, copy paste auth token string.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_AutoModel_for_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Import a pre-trained AutoModel object for a specific task. — hf_load_AutoModel_for_task","text":"AutoModel object specific task","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Load Dataset via Hugging Face Datasets Server API — hf_load_dataset","title":"Load Dataset via Hugging Face Datasets Server API — hf_load_dataset","text":"Load dataset Hugging Face Hub using Datasets Server API. API-first approach require Python. local dataset loading Python, see legacy function advanced vignette.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load Dataset via Hugging Face Datasets Server API — hf_load_dataset","text":"","code":"hf_load_dataset(   dataset,   split = \"train\",   config = NULL,   limit = 1000,   offset = 0,   token = NULL )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load Dataset via Hugging Face Datasets Server API — hf_load_dataset","text":"dataset Character string. Dataset name (e.g., \"imdb\", \"squad\"). split Character string. Dataset split: \"train\", \"test\", \"validation\", etc. Default: \"train\". config Character string NULL. Dataset configuration/subset name. NULL (default), auto-detected dataset's available configs. limit Integer. Maximum number rows fetch. Default: 1000. Set Inf fetch rows (may slow large datasets). offset Integer. Row offset pagination. Default: 0. token Character string NULL. API token private datasets.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load Dataset via Hugging Face Datasets Server API — hf_load_dataset","text":"tibble dataset rows, plus .dataset .split columns.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load Dataset via Hugging Face Datasets Server API — hf_load_dataset","text":"","code":"if (FALSE) { # \\dontrun{ # Load first 1000 rows of IMDB train set imdb <- hf_load_dataset(\"imdb\", split = \"train\", limit = 1000)  # Load test set imdb_test <- hf_load_dataset(\"imdb\", split = \"test\", limit = 500) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a pre-trained AutoModel object from Hugging Face — hf_load_model","title":"Load a pre-trained AutoModel object from Hugging Face — hf_load_model","text":"Load pre-trained AutoModel object Hugging Face","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a pre-trained AutoModel object from Hugging Face — hf_load_model","text":"","code":"hf_load_model(model_id, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a pre-trained AutoModel object from Hugging Face — hf_load_model","text":"model_id model_id id model given url https://huggingface.co/model_name. ... sent AutoModel.from_pretrained()","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a pre-trained AutoModel object from Hugging Face — hf_load_model","text":"pre-trained model object","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load a pre-trained AutoModel object from Hugging Face — hf_load_model","text":"","code":"if (FALSE) { # \\dontrun{ model <- hf_load_model(\"distilbert-base-uncased\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a pipeline object from Hugging Face - pipelines usually include a model, tokenizer and task. — hf_load_pipeline","title":"Load a pipeline object from Hugging Face - pipelines usually include a model, tokenizer and task. — hf_load_pipeline","text":"Load Model Hugging Face","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a pipeline object from Hugging Face - pipelines usually include a model, tokenizer and task. — hf_load_pipeline","text":"","code":"hf_load_pipeline(model_id, tokenizer = NULL, task = NULL, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a pipeline object from Hugging Face - pipelines usually include a model, tokenizer and task. — hf_load_pipeline","text":"model_id id model given url https://huggingface.co/model_name. tokenizer tokenizer function used tokenize inputs. Defaults NULL (one automatically loaded). task task model accomplish. Run hf_list_tasks() options. ... Fed hf_pipeline function","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_pipeline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a pipeline object from Hugging Face - pipelines usually include a model, tokenizer and task. — hf_load_pipeline","text":"Hugging Face model ready prediction.","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_sentence_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a Sentence Transformers model to extract sentence/document embeddings — hf_load_sentence_model","title":"Load a Sentence Transformers model to extract sentence/document embeddings — hf_load_sentence_model","text":"Load Sentence Transformers model extract sentence/document embeddings","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_sentence_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a Sentence Transformers model to extract sentence/document embeddings — hf_load_sentence_model","text":"","code":"hf_load_sentence_model(model_id, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_sentence_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a Sentence Transformers model to extract sentence/document embeddings — hf_load_sentence_model","text":"model_id id sentence-transformers model. Use hf_search_models(author = 'sentence-transformers') find suitable models. ... Sent model call, include arguments use_auth_token = auth_token, device = device etc.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_sentence_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a Sentence Transformers model to extract sentence/document embeddings — hf_load_sentence_model","text":"Huggingface model ready prediction.","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_sentence_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load a Sentence Transformers model to extract sentence/document embeddings — hf_load_sentence_model","text":"","code":"if (FALSE) { # \\dontrun{ # Compute sentence embeddings sentences <- c(\"Baby turtles are so cute!\", \"He walks as slowly as a turtle.\") sentences_two <- c(\"The lake is cold today.\", \"I enjoy swimming in the lake.\") sentences <- c(sentences, sentences_two) model <- hf_load_sentence_model('paraphrase-MiniLM-L6-v2') embeddings <- model$encode(sentences) embeddings %>% dist() %>% as.matrix() %>% as.data.frame() %>% setNames(sentences) embddings <- embeddings %>% dplyr::mutate(`sentence 1` = sentences) %>% tidyr::pivot_longer(cols = -`sentence 1`, names_to = 'sentence 2', values_to = 'distance') embeddings <- embeddings %>% filter(distance > 0) # Cluster sentences embeddings <- embeddings %>% t() %>% prcomp() %>% purrr::pluck('rotation') %>% as.data.frame() %>% dplyr::mutate(sentence = sentences) plot <- embedidings %>% ggplot2::ggplot(aes(PC1, PC2)) + ggplot2::geom_label(ggplot2::aes(PC1, PC2, label = sentence, vjust=\"inward\", hjust=\"inward\")) + ggplot2::theme_minimal() } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_tokenizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Load an AutoTokenizer from a pre-tained model — hf_load_tokenizer","title":"Load an AutoTokenizer from a pre-tained model — hf_load_tokenizer","text":"Load Tokenizer Hugging Face Model","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_tokenizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load an AutoTokenizer from a pre-tained model — hf_load_tokenizer","text":"","code":"hf_load_tokenizer(model_id, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_tokenizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load an AutoTokenizer from a pre-tained model — hf_load_tokenizer","text":"model_id id model given url https://huggingface.co/model_name. ... sent `AutoTokenizer.from_pretained()`, accepts named arguments e.g. use_fast https://huggingface.co/docs/transformers/main_classes/tokenizer","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_load_tokenizer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load an AutoTokenizer from a pre-tained model — hf_load_tokenizer","text":"Hugging Face model's tokenizer.","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_model_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Model Information — hf_model_info","title":"Get Model Information — hf_model_info","text":"Retrieve detailed information specific model.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_model_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Model Information — hf_model_info","text":"","code":"hf_model_info(model_id, token = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_model_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Model Information — hf_model_info","text":"model_id Character string. model ID (e.g., \"bert-base-uncased\"). token Character string NULL. API token authentication.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_model_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Model Information — hf_model_info","text":"list detailed model information.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_model_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Model Information — hf_model_info","text":"","code":"if (FALSE) { # \\dontrun{ # Get model details hf_model_info(\"sentence-transformers/all-MiniLM-L6-v2\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_nearest_neighbors.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Nearest Neighbors by Semantic Similarity — hf_nearest_neighbors","title":"Find Nearest Neighbors by Semantic Similarity — hf_nearest_neighbors","text":"Find k similar texts query text based embedding similarity.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_nearest_neighbors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Nearest Neighbors by Semantic Similarity — hf_nearest_neighbors","text":"","code":"hf_nearest_neighbors(   data,   query,   k = 5,   text_col = \"text\",   model = \"BAAI/bge-small-en-v1.5\",   token = NULL )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_nearest_neighbors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Nearest Neighbors by Semantic Similarity — hf_nearest_neighbors","text":"data data frame 'embedding' column (hf_embed_text). query Character string. query text compare . k Integer. Number nearest neighbors return. Default: 5. text_col Character string. Name text column. Default: \"text\". model Character string. Model use query embedding. match model used data embeddings. token Character string NULL. API token authentication.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_nearest_neighbors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Nearest Neighbors by Semantic Similarity — hf_nearest_neighbors","text":"tibble k nearest neighbors, sorted similarity (descending).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_nearest_neighbors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Nearest Neighbors by Semantic Similarity — hf_nearest_neighbors","text":"","code":"if (FALSE) { # \\dontrun{ docs_embedded |>   hf_nearest_neighbors(\"machine learning\", k = 5) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_parse_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse API Response to Tibble — hf_parse_response","title":"Parse API Response to Tibble — hf_parse_response","text":"Internal helper convert API JSON responses tibbles.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_parse_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse API Response to Tibble — hf_parse_response","text":"","code":"hf_parse_response(resp, input_text = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_parse_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse API Response to Tibble — hf_parse_response","text":"resp response object httr2. input_text Optional. original input text(s) include output.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_parse_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse API Response to Tibble — hf_parse_response","text":"tibble parsed results.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_python_depends.html","id":null,"dir":"Reference","previous_headings":"","what":"Install Python Dependencies — hf_python_depends","title":"Install Python Dependencies — hf_python_depends","text":"Installs python packages needed run huggingfaceR functions","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_python_depends.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install Python Dependencies — hf_python_depends","text":"","code":"hf_python_depends(   packages = c(\"transformers\", \"sentencepiece\", \"huggingface_hub\", \"datasets\",     \"sentence-transformers\") )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_python_depends.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install Python Dependencies — hf_python_depends","text":"packages Python libraries needed local model usage.  Defaults transformers, sentencepiece, huggingface_hub, datasets, sentence-transformers.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_question_answering_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Question Answering Payload — hf_question_answering_payload","title":"Question Answering Payload — hf_question_answering_payload","text":"Want nice know--bot can answer question?","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_question_answering_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Question Answering Payload — hf_question_answering_payload","text":"","code":"hf_question_answering_payload(question, context)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_question_answering_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question Answering Payload — hf_question_answering_payload","text":"question question answered based provided context context context consult answering question","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_question_answering_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Question Answering Payload — hf_question_answering_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_resolve_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Resolve a Short Dataset Name to Full ID — hf_resolve_dataset","title":"Resolve a Short Dataset Name to Full ID — hf_resolve_dataset","text":"Uses Hub API find full org/name ID dataset.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_resolve_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resolve a Short Dataset Name to Full ID — hf_resolve_dataset","text":"","code":"hf_resolve_dataset(dataset, token = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_resolve_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resolve a Short Dataset Name to Full ID — hf_resolve_dataset","text":"dataset Character string. Short dataset name. token Character string NULL. API token.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_resolve_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resolve a Short Dataset Name to Full ID — hf_resolve_dataset","text":"Character string full dataset ID, NULL found.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_datasets.html","id":null,"dir":"Reference","previous_headings":"","what":"Search Datasets on Hugging Face Hub — hf_search_datasets","title":"Search Datasets on Hugging Face Hub — hf_search_datasets","text":"Search datasets using various filters.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_datasets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search Datasets on Hugging Face Hub — hf_search_datasets","text":"","code":"hf_search_datasets(   search = NULL,   task = NULL,   language = NULL,   size = NULL,   sort = \"downloads\",   limit = 30,   token = NULL )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_datasets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search Datasets on Hugging Face Hub — hf_search_datasets","text":"search Character string. Search query filter datasets. task Character string. Filter task. language Character string. Filter language. size Character string. Filter size: \"small\", \"medium\", \"large\". sort Character string. Sort : \"downloads\", \"likes\", \"created\", \"updated\". Default: \"downloads\". limit Integer. Maximum number datasets return. Default: 30. token Character string NULL. API token authentication.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_datasets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search Datasets on Hugging Face Hub — hf_search_datasets","text":"tibble dataset information.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_datasets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search Datasets on Hugging Face Hub — hf_search_datasets","text":"","code":"if (FALSE) { # \\dontrun{ # Search datasets hf_search_datasets(search = \"sentiment\", limit = 10) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Search Models on Hugging Face Hub — hf_search_models","title":"Search Models on Hugging Face Hub — hf_search_models","text":"Search models using various filters. Returns tibble matching models.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search Models on Hugging Face Hub — hf_search_models","text":"","code":"hf_search_models(   search = NULL,   task = NULL,   author = NULL,   language = NULL,   library = NULL,   tags = NULL,   sort = \"downloads\",   direction = \"desc\",   limit = 30,   token = NULL )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search Models on Hugging Face Hub — hf_search_models","text":"search Character string. Search query filter models. task Character string. Filter task (e.g., \"text-classification\"). author Character string. Filter model author/organization. language Character string. Filter language (e.g., \"en\"). library Character string. Filter library (e.g., \"pytorch\", \"transformers\"). tags Character vector. Filter tags. sort Character string. Sort field: \"downloads\", \"likes\", \"created\", \"updated\". Default: \"downloads\". direction Character string. Sort direction: \"asc\" \"desc\". Default: \"desc\". limit Integer. Maximum number models return. Default: 30. token Character string NULL. API token authentication.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search Models on Hugging Face Hub — hf_search_models","text":"tibble model information.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_search_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search Models on Hugging Face Hub — hf_search_models","text":"","code":"if (FALSE) { # \\dontrun{ # Search by task hf_search_models(task = \"text-classification\", limit = 10)  # Search by author hf_search_models(author = \"facebook\", sort = \"downloads\")  # Search with query hf_search_models(search = \"sentiment\", task = \"text-classification\") } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_encode.html","id":null,"dir":"Reference","previous_headings":"","what":"Use a Sentence Transformers pipeline to extract document(s)/sentence(s) embedding(s) — hf_sentence_encode","title":"Use a Sentence Transformers pipeline to extract document(s)/sentence(s) embedding(s) — hf_sentence_encode","text":"Use Sentence Transformers pipeline extract document(s)/sentence(s) embedding(s)","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_encode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use a Sentence Transformers pipeline to extract document(s)/sentence(s) embedding(s) — hf_sentence_encode","text":"","code":"hf_sentence_encode(   model,   text,   batch_size = 64L,   show_progress_bar = TRUE,   tidy = TRUE,   ... )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_encode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use a Sentence Transformers pipeline to extract document(s)/sentence(s) embedding(s) — hf_sentence_encode","text":"model Model object loaded `hf_load_sentence_model()` text text, texts, wish embed/encode. batch_size many texts embed . show_progress_bar Whether print progress bar console . tidy Whether tidy output tibble . ... args sent model's encode method, e.g. device = device","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_encode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use a Sentence Transformers pipeline to extract document(s)/sentence(s) embedding(s) — hf_sentence_encode","text":"n-dimensional embeddings every input `text`","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_encode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use a Sentence Transformers pipeline to extract document(s)/sentence(s) embedding(s) — hf_sentence_encode","text":"","code":"if (FALSE) { # \\dontrun{ text <- c(\"There are things we do know, things we don't know, and then there is quantum mechanics.\") sentence_mod <- hf_load_sentence_model(\"paraphrase-MiniLM-L6-v2\") embeddings <- hf_sentence_encode(model = sentence_mod, text, show_progress_bar = TRUE) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_similarity_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Sentence Similarity Payload — hf_sentence_similarity_payload","title":"Sentence Similarity Payload — hf_sentence_similarity_payload","text":"Calculate semantic similarity one text list sentences comparing embeddings.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_similarity_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sentence Similarity Payload — hf_sentence_similarity_payload","text":"","code":"hf_sentence_similarity_payload(source_sentence, sentences)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_similarity_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sentence Similarity Payload — hf_sentence_similarity_payload","text":"source_sentence string wish compare strings . can phrase, sentence, longer passage, depending model used. sentences list strings compared source_sentence.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_sentence_similarity_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sentence Similarity Payload — hf_sentence_similarity_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_device.html","id":null,"dir":"Reference","previous_headings":"","what":"Try to set device to GPU for accelerated computation — hf_set_device","title":"Try to set device to GPU for accelerated computation — hf_set_device","text":"function currently depends working installation torch GPU environment. running Apple silicon GPU, need native mac M+ build (ARM binary). also need rust transformers dependencies. need make sure everything needs GPU (tensors, model, pipeline etc.), GPU, currently recommend advanced users . working integrating fully installation build huggingfaceR environment.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_device.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Try to set device to GPU for accelerated computation — hf_set_device","text":"","code":"hf_set_device()"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_device.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Try to set device to GPU for accelerated computation — hf_set_device","text":"device models, pipelines, tensors can sent .","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_device.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Try to set device to GPU for accelerated computation — hf_set_device","text":"","code":"if (FALSE) { # \\dontrun{ device <- hf_set_device() } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Set Hugging Face API Token — hf_set_token","title":"Set Hugging Face API Token — hf_set_token","text":"Set update Hugging Face API token authentication. token can obtained https://huggingface.co/settings/tokens","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set Hugging Face API Token — hf_set_token","text":"","code":"hf_set_token(token = NULL, store = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set Hugging Face API Token — hf_set_token","text":"token Character string containing HF token, NULL set interactively. NULL, prompt token input (echoed console). store Logical. TRUE, stores token .Renviron future sessions. Default: FALSE (token available current session).","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_token.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set Hugging Face API Token — hf_set_token","text":"Invisibly returns TRUE token set successfully.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_set_token.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set Hugging Face API Token — hf_set_token","text":"","code":"if (FALSE) { # \\dontrun{ # Set token for current session only hf_set_token(\"hf_xxxxxxxxxxxxx\")  # Set token interactively and store permanently hf_set_token(store = TRUE) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Pairwise Similarity — hf_similarity","title":"Compute Pairwise Similarity — hf_similarity","text":"Compute cosine similarity pairs embeddings.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Pairwise Similarity — hf_similarity","text":"","code":"hf_similarity(embeddings, text_col = \"text\")"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Pairwise Similarity — hf_similarity","text":"embeddings tibble 'embedding' column (hf_embed). text_col Character string. Name text column. Default: \"text\".","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Pairwise Similarity — hf_similarity","text":"tibble columns: text_1, text_2, similarity","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Pairwise Similarity — hf_similarity","text":"","code":"if (FALSE) { # \\dontrun{ sentences <- c(\"I love cats\", \"I adore felines\", \"Dogs are great\") embeddings <- hf_embed(sentences) similarities <- hf_similarity(embeddings) } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_summarization_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarization Task Payload — hf_summarization_payload","title":"Summarization Task Payload — hf_summarization_payload","text":"task well known summarize longer text shorter text. careful, models maximum length input. means summary handle full books instance.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_summarization_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarization Task Payload — hf_summarization_payload","text":"","code":"hf_summarization_payload(   string,   min_length = NULL,   max_length = NULL,   top_k = NULL,   top_p = NULL,   temperature = 1,   repetition_penalty = NULL,   max_time = NULL )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_summarization_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarization Task Payload — hf_summarization_payload","text":"string string summarized min_length Integer define minimum length tokens output summary. Default: NULL max_length Integer define maximum length tokens output summary. Default: NULL top_k Integer define top tokens considered within sample operation create new text. Default: NULL top_p Float define tokens within sample operation text generation. Add tokens sample probable least probable sum probabilities greater top_p. Default: NULL temperature Float (0.0-100.0). temperature sampling operation. 1 means regular sampling, 0 means always take highest score, 100.0 getting closer uniform probability. Default: 1.0 repetition_penalty Float (0.0-100.0). token used within generation penalized picked successive generation passes. Default: NULL max_time Float (0-120.0). amount time seconds query take maximum. Network can cause overhead soft limit. Default: NULL","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_summarization_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarization Task Payload — hf_summarization_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_table_question_answering_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Table Question Answering Payload — hf_table_question_answering_payload","title":"Table Question Answering Payload — hf_table_question_answering_payload","text":"Don’t know SQL? Don’t want dive large spreadsheet? Ask questions plain english!","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_table_question_answering_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Table Question Answering Payload — hf_table_question_answering_payload","text":"","code":"hf_table_question_answering_payload(query, table)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_table_question_answering_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Table Question Answering Payload — hf_table_question_answering_payload","text":"query query plain text want ask table table table data represented dict list entries headers lists values, lists must size.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_table_question_answering_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Table Question Answering Payload — hf_table_question_answering_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text2text_generation_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Text2Text Generation Payload — hf_text2text_generation_payload","title":"Text2Text Generation Payload — hf_text2text_generation_payload","text":"takes input containing sentence including task returns output accomplished task.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text2text_generation_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text2Text Generation Payload — hf_text2text_generation_payload","text":"","code":"hf_text2text_generation_payload(string)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text2text_generation_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text2Text Generation Payload — hf_text2text_generation_payload","text":"string string containing question task sentence answer derived","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text2text_generation_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text2Text Generation Payload — hf_text2text_generation_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text_classification_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Classification Payload — hf_text_classification_payload","title":"Text Classification Payload — hf_text_classification_payload","text":"Usually used sentiment-analysis output likelihood classes input.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text_classification_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Classification Payload — hf_text_classification_payload","text":"","code":"hf_text_classification_payload(string)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text_classification_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Classification Payload — hf_text_classification_payload","text":"string string classified","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text_classification_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text Classification Payload — hf_text_classification_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text_generation_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Generation Payload — hf_text_generation_payload","title":"Text Generation Payload — hf_text_generation_payload","text":"Use continue text prompt. generic task.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text_generation_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Generation Payload — hf_text_generation_payload","text":"","code":"hf_text_generation_payload(   string,   top_k = NULL,   top_p = NULL,   temperature = 1,   repetition_penalty = NULL,   max_new_tokens = NULL,   max_time = NULL,   return_full_text = TRUE,   num_return_sequences = 1L,   do_sample = TRUE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text_generation_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Generation Payload — hf_text_generation_payload","text":"string string generated top_k (Default: None). Integer define top tokens considered within sample operation create new text. top_p (Default: None). Float define tokens within sample operation text generation. Add tokens sample probable least probable sum probabilities greater top_p temperature Float (0.0-100.0). temperature sampling operation. 1 means regular sampling, 0 means always take highest score, 100.0 getting closer uniform probability. Default: 1.0 repetition_penalty (Default: None). Float (0.0-100.0). token used within generation penalized picked successive generation passes. max_new_tokens (Default: None). Int (0-250). amount new tokens generated, include input length estimate size generated text want. new tokens slows request, look balance response times length text generated. max_time (Default: None). Float (0-120.0). amount time seconds query take maximum. Network can cause overhead soft limit. Use combination max_new_tokens best results. return_full_text (Default: True). Bool. set False, return results contain original query making easier prompting. num_return_sequences (Default: 1). Integer. number proposition want returned. do_sample (Optional: True). Bool. Whether use sampling, use greedy decoding otherwise.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_text_generation_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text Generation Payload — hf_text_generation_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_token_classification_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Token Classification Payload — hf_token_classification_payload","title":"Token Classification Payload — hf_token_classification_payload","text":"Usually used sentence parsing, either grammatical, Named Entity Recognition (NER) understand keywords contained within text.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_token_classification_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Token Classification Payload — hf_token_classification_payload","text":"","code":"hf_token_classification_payload(string, aggregation_strategy = \"simple\")"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_token_classification_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Token Classification Payload — hf_token_classification_payload","text":"string string classified aggregation_strategy (Default: simple). several aggregation strategies.  none: Every token gets classified without aggregation.   simple: Entities grouped according default schema (B-, - tags get merged tag similar).   first: simple strategy except words end different tags. Words use tag first token ambiguity.   average: simple strategy except words end different tags. Scores averaged across tokens maximum label applied.   max: simple strategy except words end different tags. Word entity token maximum score.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_token_classification_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Token Classification Payload — hf_token_classification_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_translation_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Translation Payload — hf_translation_payload","title":"Translation Payload — hf_translation_payload","text":"task well known translate text one language another","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_translation_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Translation Payload — hf_translation_payload","text":"","code":"hf_translation_payload(string)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_translation_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Translation Payload — hf_translation_payload","text":"string string translated original languages","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_translation_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Translation Payload — hf_translation_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/hf_vectorize.html","id":null,"dir":"Reference","previous_headings":"","what":"Vectorize API Calls — hf_vectorize","title":"Vectorize API Calls — hf_vectorize","text":"Internal helper apply API function multiple inputs efficiently.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_vectorize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vectorize API Calls — hf_vectorize","text":"","code":"hf_vectorize(inputs, fn, ..., .progress = FALSE)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_vectorize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vectorize API Calls — hf_vectorize","text":"inputs Character vector inputs. fn Function apply input. ... Additional arguments passed fn. .progress Logical. Show progress bar?","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_vectorize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vectorize API Calls — hf_vectorize","text":"Combined results API calls.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_whoami.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Current Hugging Face User Information — hf_whoami","title":"Get Current Hugging Face User Information — hf_whoami","text":"Retrieve information currently authenticated user. Requires valid Hugging Face token set.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_whoami.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Current Hugging Face User Information — hf_whoami","text":"","code":"hf_whoami(token = NULL)"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_whoami.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Current Hugging Face User Information — hf_whoami","text":"token Character string containing HF token. NULL, uses HUGGING_FACE_HUB_TOKEN environment variable.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_whoami.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Current Hugging Face User Information — hf_whoami","text":"tibble user information including name, email, organizations.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_whoami.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Current Hugging Face User Information — hf_whoami","text":"","code":"if (FALSE) { # \\dontrun{ # Check current user hf_whoami() } # }"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_zero_shot_classification_payload.html","id":null,"dir":"Reference","previous_headings":"","what":"Zero Shot Classification Payload — hf_zero_shot_classification_payload","title":"Zero Shot Classification Payload — hf_zero_shot_classification_payload","text":"task super useful try classification zero code, simply pass sentence/paragraph possible labels sentence, get result.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_zero_shot_classification_payload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Zero Shot Classification Payload — hf_zero_shot_classification_payload","text":"","code":"hf_zero_shot_classification_payload(   string,   candidate_labels,   multi_label = FALSE )"},{"path":"https://farach.github.io/huggingfaceR/reference/hf_zero_shot_classification_payload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Zero Shot Classification Payload — hf_zero_shot_classification_payload","text":"string string list strings candidate_labels list strings potential classes inputs. (max 10 candidate_labels, , simply run multiple requests, results going misleading using many candidate_labels anyway. want keep exact , can simply run multi_label=True scaling end. ) multi_label (Default: false) Boolean set True classes can overlap","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/hf_zero_shot_classification_payload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Zero Shot Classification Payload — hf_zero_shot_classification_payload","text":"inference payload","code":""},{"path":[]},{"path":"https://farach.github.io/huggingfaceR/reference/models_with_downloads.html","id":null,"dir":"Reference","previous_headings":"","what":"Dataset of model names, tasks & download info — models_with_downloads","title":"Dataset of model names, tasks & download info — models_with_downloads","text":"model name model downloads number downloads task task model performsl sha model's secure hash algorithm private whether model public private","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/models_with_downloads.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset of model names, tasks & download info — models_with_downloads","text":"","code":"models_with_downloads"},{"path":"https://farach.github.io/huggingfaceR/reference/models_with_downloads.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dataset of model names, tasks & download info — models_with_downloads","text":"object class tbl_df (inherits tbl, data.frame) 55524 rows 5 columns.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://farach.github.io/huggingfaceR/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling `rhs(lhs)`.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/step_hf_embed.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedding Recipe Step — step_hf_embed","title":"Embedding Recipe Step — step_hf_embed","text":"Create text embeddings using Hugging Face model part tidymodels recipe. step converts text columns embedding features downstream modeling.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/step_hf_embed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedding Recipe Step — step_hf_embed","text":"","code":"step_hf_embed(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   model = \"BAAI/bge-small-en-v1.5\",   token = NULL,   embeddings = NULL,   skip = FALSE,   id = recipes::rand_id(\"hf_embed\") )  # S3 method for class 'step_hf_embed' tidy(x, ...)  # S3 method for class 'step_hf_embed' tunable(x, ...)"},{"path":"https://farach.github.io/huggingfaceR/reference/step_hf_embed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedding Recipe Step — step_hf_embed","text":"recipe recipe object. ... One text column selectors (see recipes::selections()). role Character string. Role new embedding variables. Default: \"predictor\". trained Logical. Internal use . model Character string. Hugging Face model ID embeddings. Default: \"BAAI/bge-small-en-v1.5\". token Character string NULL. API token authentication. embeddings List. Internal use (stores embeddings training). skip Logical. step skipped baking? Default: FALSE. id Character string. Unique ID step. x step_hf_embed object","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/step_hf_embed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embedding Recipe Step — step_hf_embed","text":"updated recipe object.","code":""},{"path":"https://farach.github.io/huggingfaceR/reference/step_hf_embed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Embedding Recipe Step — step_hf_embed","text":"","code":"if (FALSE) { # \\dontrun{ library(tidymodels) library(dplyr)  # Create a recipe with embeddings rec <- recipe(sentiment ~ text, data = train_data) |>   step_hf_embed(text, model = \"BAAI/bge-small-en-v1.5\")  # Use in a workflow wf <- workflow() |>   add_recipe(rec) |>   add_model(logistic_reg()) |>   fit(data = train_data) } # }"}]
